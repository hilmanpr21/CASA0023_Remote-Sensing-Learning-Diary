[
  {
    "objectID": "week01.html#summary",
    "href": "week01.html#summary",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote sensing is acquiring data or information from a distance. In this module, context we use remote sensing for Earth Observation (EO). This can be done by using sensor mounted on satelite.\nIn this week we will talk general topic about sensor and satellite on on earth observation. It can be seen in the mind map that there will be 4 subtopics to make us easier understanding Sensor, which are:\n\nTypes of Sensor\n\nPassive Sensor\nActive Sensor\n\nElectromagnetic waves or radiation (EMR)\n\nEMR interaction with atmosphere before hitting sensor\nEMR interaction with earth surface before hitting sensor\n\nType or satellite orbit\n\nGeosynchronous Orbit\nGeostationary Orbit\n\nData produced by sensor\n\nRaster Data Format\nResolution\n\nSpectral\nSpatial\nTemporal\nRadiometric\n\n\n\nCheck the mind map to understand the outline of this week’s topic\n\n\n\nMind map\n\n\n\n1.1.1 Types of Sensor\n\n1.1.1.1 Passive Sensor\n\nUse electromagnetic waves from sun-ray as source of energy.\nDetect reflected energy from the sun that has different wavelength to detect different colour spectrum\nSuch as:\n\nsentinel\nlandsat \n\n\n\n\n1.1.1.2 Active Sensor\n\nHave an energy source for Illumination\nActively emits electromagnetics wave and then wait to rceive\nSuch as:\n\nSynthetic-aperture Radar (SAR),\nX-ray,\nLiDAR\n\n\n\n\nActive Sensor\n\n\n\n\n\n\n1.1.2 Electro Magnetic waves\n\nis waves that have both eletric and magentic field, produce by vibration of particles\nEM waves move in vacuum\nPerpendicular electric and magnetic field\ndifferent wavelength causes different colour spectrum. That’s why we can see colourful image\n\n\n\n\nElectromagnetive Wave\n\n\nsource: Principle of Remote Sensing\n\n1.1.2.1 Terms\n\nElectromagnetic radiation (EMR) = Waves of an electromagnetic field, travel through space and carry radiant energy\nRadiant Energy = energy carried by EMR Waves\nRadian flux = Energy per unit of time\nExitance emittance (per unit time\nflux) = energy leaving a surface per unit area per unit time - Flux = time\n\n\n\n1.1.2.2 Electromagnetric Radiaton (EMR) interaction\n\nBefore hitting hitting the sensor, energy from electromagnetic radiation will interacting with:\n\natmosphere\n\nscattered by atmosphere particle\n\nearth’s surface\n\nabsorbed\ntransmitted\n\n\n\n\n1.1.2.2.1 Atmospheric Scattering\n\nElectromagnetic waves are scatters by particles on the atmosphere\n\n\n\n\nElectromagnetive Wave\n\n\nsource: Word is inMocean\n\nSmaller wavelengths scatter easier\nBlue has the smallest wavelength\n\n\n\n\nRGB Wavelength\n\n\n\ntype of scattering:\n\nRayleigh is when particles are very small compared to the wavelength.\n\neg: sky looks blue because sunray scattered by particles. in sunset or sunrise is much more atmosphere to pass throught so more scattered. When sun’s angle change the blue light scatter does not reach our eyes as the distance is increased, so longer wavelength like red and orange reach us\n\nMie is when particles are same size compared to the wavelength\nNon Selective is when particles are much larger compared to the wavelength\n\ndrawback of passive sensor is that it could not pass through cloud\n\n\n\n1.1.2.2.2 Bidirectional Documented Surface Interaction (BDRF)\n\nhappens because sensor and illumination (sun) angles can change.\nIt can cause\n\nbackscattering (left): sun behind the sensor, bright region at sensor and sun side\nforwards scattering (right): sun opposite satelite, shadow region at sensor side\n\n\n\n\nBack Scattering and Forward Scattering\n\n\nsource: Professor Crystal Schaaf’s Lab\n\n\n“There are many interaction that influence the data being created because it’s not merely say that energy from sun is reflected by earth to sensor (but there are may factor such as scattering and interaction with surface)”\n\n\n\n\n\n1.1.3 Types of orbit\n\nGeosynchronous Orbit (GSO)\n\nsatelite matches the earth’s rotation, usually for sensor\n\nGeostationary Orbit\n\nsatelite holds same position, usually only for communication\n\n\n\n\nOrbit type\n\n\nsource: American Scientist\n\n\n\n1.1.4 Data\n\n1.1.4.1 Raster Format\n\nSatellite sensor produces raster imagery data. Raster image is a graphic repesented as a rectangular matrix or grid of square pixels.\n\n\n\n\nRaster Grid\n\n\nsource: QGIS\n\n\n1.1.4.2 Four Data Resolution\n\n1.1.4.2.1 Spatial\n\nSpatial resolution is the size of the raster grid per pixel. The smaller the grid means the higher the resolution be so the clearer the image be\n\nsmallest grid is 10cm\n\n\n\n\n1.1.4.2.2 Temporal\n\nTemporal resolution is showing how often the data being updated\n\n\n\n1.1.4.2.3 Radiometric\n\nRadiometric is identified differences in light or reflectance, in practice this is the range of possible values. or the ability od a sensor to identify and show small difference in energy.\nThe higher the bit, the more sensitive and the more information\nhowever, the lower the radiometric resolution, the higher possibility to differentiate features\n\n\n\n1.1.4.2.4 Spectral\n\nthis concept similar to pixel concept in digital camera where different colour (called band) is stacked together so it makes a new colour combination.\n\n\n\n\npixel band system\n\n\nsource: The hype in spectral imaging | Spectroscopy Europe/World\n\nspectral resolution based on number of bands being observed\n\n\n\n\nband list\n\n\n\nWe can take values for each wavelength (or a band of several wavelengths) across the electromagnetic spectrum to create a spectral signature. different feature on earth has different spectral signature, it will be used to identify different object. spectral signature can be discrete (eg. multispectral) or continuous (eg. hyperspectral)\n\n\n\n\nSpectral Signature\n\n\nsource: Nireos\n\nThis spectral signature are used to differentiate object in the image. In earth observation we use this for identifying land cover in earth surface using Normalized Difference Vegetation Index (NDVI), Normalized Difference Building Index (NDBI)\nfor example in this week practical we identify the landcover by making scatter plot with band 4 (red, vegetation absorb) and band 8 (Near-Infrared, NIR, that vegetation strongly reflect). high values of NIR and low values of red represents dense vegetation. Meanwhile low values of red and NIR represent dense vegetation\n\n\n\n\nScatter Plot band 4 and band 8"
  },
  {
    "objectID": "week01.html#application",
    "href": "week01.html#application",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application\nThe increasing satellite remote sensing data on earth observation in the last two decades, allows new approaches on understanding urban setting (Miller and Small, 2003). Satellite remote sensing data offers some advantages, such as: - broader spatial coverage, - ability for routine update, - consistent measurement\nCombination of remote sensing data with census data or other government data can provide us useful information such as, urban growth, urban heat island, and socioeconomic activities. We will discuss some application examples of remote sensing data, classified based on sensor type.\n\n1.2.1 Active Sensor\n\n1.2.1.1 Population Size estimation using Synthetic Aperture Radar (SAR)\nDifferent from passive sensor, Synthetic Aperture Radar (SAR) is an active sensor that can see through cloud and the backscatter radar wave can determine physical properties such as density and surface texture (Henderson and Zong-Guo Xia, Jan./1997). The ability of SAR to detect horizontal and vertical structure allows to detect settlement based on texture, materials geometry and density. The result of settlement detection can be used for estimating city population by multiplying with number of occupants per dwelling. Harris Harris (1985) used this methods for estimating population in Tunisia.\nThis methods is beneficial for developing countries because they have more informal types of dwellings. Radar Data is really helpful because population census happens only once in 10 years Henderson and Zong-Guo Xia (Jan./1997) which could not keep up the rapid urban growth.\n\n\n\n1.2.2 Passive Sensor\n\n1.2.2.1 Evaluating Poverty using Nighttime Light (NTL) Satellite\nNighttime Light Satellite is a passive sensor launched by nasa to collect data of night light emission (Earth Science Data Systems, 2021). The data from this satellite provide unique perspective that can be beneficial for detecting global conflict, human night behaviour, ecological impact of artificial night light and so on.\nOne of the benefit of nighttime light data is to track socioeconomic inequality and poverty. Yu et al. (2015) evaluated poverty rate in 2856 counties in china. The research compare the average light index from Visible Infrared day-night band with integrated poverty index. It used linear regression and comparison of class to check how accurate the remote sensing data to evaluate poverty. The result shows that day-night band data can be useful to evaluate poverty in china"
  },
  {
    "objectID": "week01.html#reflection",
    "href": "week01.html#reflection",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nThe increase of remote sensing data on earth observation helps researcher, policy makers, and local government to understand the urban context better. Special characteristic of remote sensing data allows us to have broader perspectives and collect spatial improvement from time to time which could not be provided by tradisional population census data.\nIn the application we can combine the remote sensing data with census data to have more robust analysis.\nAlthough, using remote sensing data looks more fancy, in some cases and context we can get the similar data that are more efficient and cost effective from governmental source (Miller and Small, 2003). For example we can use building permission data rather than satellite data to identify land use.\nMoreover, managing remote sensing data require higher skill than traditional data. It can be an obstacle for cities that has limited resources\n\n\n\n\n\nEarth Science Data Systems, N. (2021) Nighttime Lights. Earthdata; Earth Science Data Systems, NASA. Available at: https://www.earthdata.nasa.gov/learn/backgrounders/nighttime-lights (Accessed: February 15, 2023).\n\n\nHarris, R. (1985) “SIR-A imagery of Tunisia and its potential for population estimation,” International Journal of Remote Sensing, 6(7), pp. 975–978. doi: 10.1080/01431168508948260.\n\n\nHenderson, F. M. and Zong-Guo Xia (Jan./1997) “SAR applications in human settlement detection, population estimation and urban land use pattern analysis: A status report,” IEEE Transactions on Geoscience and Remote Sensing, 35(1), pp. 79–85. doi: 10.1109/36.551936.\n\n\nMiller, R. B. and Small, C. (2003) “Cities from space: Potential applications of remote sensing in urban environmental research and policy,” Environmental Science & Policy, 6(2), pp. 129–137. doi: 10.1016/S1462-9011(03)00002-9.\n\n\nYu, B. et al. (2015) “Poverty Evaluation Using NPP-VIIRS Nighttime Light Composite Data at the County Level in China,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 8(3), pp. 1217–1229. doi: 10.1109/JSTARS.2015.2399416."
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  week_05",
    "section": "",
    "text": "6 Summary\nIn this week we are going to learn about Google Earth Engine (GEE). We will discuss about GEE in 2 section. First, we are going to learn about brief introduction about GEE. Second we are going to discuss about one of processes we can do in GEE which is Reducing Image"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "9  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "To wrap up, this learning diary was made with aims if in the future I come across remote sensing project, I can simply open this learning Diary.\nLearn about remote sensing is really beneficial and fun because other than we learn about methodologies, we also learn about it’s application on real world context.\nThe most interesting part is on the group project assignment because we have to change our academic oriented perspective to more client-facing perspective."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Albarqouni, Mohammed M. Y., Nur Yagmur, Filiz Bektas Balcik, and\nAliihsan Sekertekin. 2022. “Assessment of Spatio-Temporal\nChanges in Water Surface Extents and Lake\nSurface Temperatures Using Google Earth Engine for Lakes\nRegion, Türkiye.” ISPRS International\nJournal of Geo-Information 11 (7, 7): 407. https://doi.org/10.3390/ijgi11070407.\n\n\nDeng, J. S., K. Wang, Y. H. Deng, and G. J. Qi. 2008.\n“PCA‐based Land‐use Change Detection and Analysis\nUsing Multitemporal and Multisensor Satellite Data.”\nInternational Journal of Remote Sensing 29 (16): 4823–38. https://doi.org/10.1080/01431160801950162.\n\n\nEarth Science Data Systems, NASA. 2021. “Nighttime\nLights.” Backgrounder. Earthdata;\nEarth Science Data Systems, NASA. May 11, 2021. https://www.earthdata.nasa.gov/learn/backgrounders/nighttime-lights.\n\n\nEckmann, T. C., D. A. Roberts, and C. J. Still. 2009. “Estimating\nSubpixel Fire Sizes and Temperatures from ASTER Using\nMultiple Endmember Spectral Mixture Analysis.” International\nJournal of Remote Sensing 30 (22): 5851–64. https://doi.org/10.1080/01431160902748531.\n\n\nHarris, Ray. 1985. “SIR-A Imagery of\nTunisia and Its Potential for Population\nEstimation.” International Journal of Remote Sensing 6\n(7): 975–78. https://doi.org/10.1080/01431168508948260.\n\n\nHenderson, F. M., and Zong-Guo Xia. Jan./1997. “SAR\nApplications in Human Settlement Detection, Population Estimation and\nUrban Land Use Pattern Analysis: A Status Report.” IEEE\nTransactions on Geoscience and Remote Sensing 35 (1): 79–85. https://doi.org/10.1109/36.551936.\n\n\nIngebritsen, S. E., and R. J. P. Lyon. 1985. “Principal Components\nAnalysis of Multitemporal Image Pairs.” International Journal\nof Remote Sensing 6 (5): 687–96. https://doi.org/10.1080/01431168508948491.\n\n\nLicciardi, Giorgio, Prashanth Reddy Marpu, Jocelyn Chanussot, and Jon\nAtli Benediktsson. 2012. “Linear Versus Nonlinear PCA\nfor the Classification of Hyperspectral Data\nBased on the Extended Morphological\nProfiles.” IEEE Geoscience and Remote Sensing\nLetters 9 (3): 447–51. https://doi.org/10.1109/LGRS.2011.2172185.\n\n\nMiller, Roberta Balstad, and Christopher Small. 2003. “Cities from\nSpace: Potential Applications of Remote Sensing in Urban Environmental\nResearch and Policy.” Environmental Science & Policy\n6 (2): 129–37. https://doi.org/10.1016/S1462-9011(03)00002-9.\n\n\nMitraka, Zina, Nektarios Chrysoulakis, Yiannis Kamarianakis, Panagiotis\nPartsinevelos, and Androniki Tsouchlaraki. 2012. “Improving the\nEstimation of Urban Surface Emissivity Based on Sub-Pixel Classification\nof High Resolution Satellite Imagery.” Remote Sensing of\nEnvironment, Remote Sensing of Urban\nEnvironments, 117 (February): 125–34. https://doi.org/10.1016/j.rse.2011.06.025.\n\n\n“Multivariate Multiple Linear Regression.”\nn.d. StatsTest.com. Accessed March 19, 2023. https://www.statstest.com/multivariate-multiple-linear-regression/.\n\n\nPérez Machado, Reinaldo, and Christopher Small. 2013.\n“IDENTIFYING MULTI-DECADAL CHANGES OF THE SAO PAULO URBAN\nAGGLOMERATION WITH MIXED REMOTE SENSING TECHNIQUES:\nSPECTRAL MIXTURE ANALYSIS AND NIGHT LIGHTS.”\nEARSeL eProceedings 12 (September): 101–12. https://doi.org/10.12760/01-2013-2-03.\n\n\nPlaza, A., P. Martinez, R. Perez, and J. Plaza. 2002.\n“Spatial/Spectral Endmember Extraction by Multidimensional\nMorphological Operations.” IEEE Transactions on Geoscience\nand Remote Sensing 40 (9): 2025–41. https://doi.org/10.1109/TGRS.2002.802494.\n\n\nRodarmel, Craig, and Jie Shan. 2002. “Principal Component\nAnalysis for Hyperspectral Image\nClassification.” Surv Land Inf Syst 62 (January).\n\n\nSantamouris, M., C. Cartalis, A. Synnefa, and D. Kolokotsa. 2015a.\n“On the Impact of Urban Heat Island and Global Warming on the\nPower Demand and Electricity Consumption of Buildings—A\nReview.” Energy and Buildings 98 (July): 119–24. https://doi.org/10.1016/j.enbuild.2014.09.052.\n\n\n———. 2015b. “On the Impact of Urban Heat Island and Global Warming\non the Power Demand and Electricity Consumption of\nBuildings—A Review.” Energy and Buildings,\nRenewable Energy Sources and Healthy\nBuildings, 98 (July): 119–24. https://doi.org/10.1016/j.enbuild.2014.09.052.\n\n\nSidiqui, Paras, Phillip B. Roös, Murray Herron, David S. Jones, Emma\nDuncan, Ali Jalali, Zaheer Allam, et al. 2022. “Urban Heat\nIsland Vulnerability Mapping Using Advanced GIS Data\nand Tools.” Journal of Earth System Science 131 (4):\n266. https://doi.org/10.1007/s12040-022-02005-w.\n\n\nTakebayashi, Hideki, and Masakazu Moriyama. 2020. “Chapter 1 -\nBackground and Purpose.” In Adaptation\nMeasures for Urban Heat Islands, edited\nby Hideki Takebayashi and Masakazu Moriyama, 1–8. Academic\nPress. https://doi.org/10.1016/B978-0-12-817624-5.00001-4.\n\n\nTan, Jianguo, Youfei Zheng, Xu Tang, Changyi Guo, Liping Li, Guixiang\nSong, Xinrong Zhen, et al. 2010. “The Urban Heat Island and Its\nImpact on Heat Waves and Human Health in Shanghai.”\nInternational Journal of Biometeorology 54 (1): 75–84. https://doi.org/10.1007/s00484-009-0256-x.\n\n\nTécher, Magalie, Hassan Ait Haddou, and Rahim Aguejdad. 2023.\n“Urban Heat Island’s Vulnerability\nAssessment by Integrating Urban Planning Policies:\nA Case Study of Montpellier Méditerranée Metropolitan\nArea, France.” Sustainability 15 (3,\n3): 1820. https://doi.org/10.3390/su15031820.\n\n\nUS EPA, OAR. 2014. “Learn About Heat Islands.”\nOverviews and Factsheets. June 17, 2014. https://www.epa.gov/heatislands/learn-about-heat-islands.\n\n\nXie, Bo, Chunxiang Cao, Min Xu, Xinwei Yang, Robert Shea Duerler,\nBarjeece Bashir, Zhibin Huang, Kaimin Wang, Yiyu Chen, and Heyi Guo.\n2022. “Improved Forest Canopy Closure Estimation Using\nMultispectral Satellite Imagery Within Google Earth\nEngine.” Remote Sensing 14 (9, 9): 2051. https://doi.org/10.3390/rs14092051.\n\n\nYu, Bailang, Kaifang Shi, Yingjie Hu, Chang Huang, Zuoqi Chen, and\nJianping Wu. 2015. “Poverty Evaluation Using NPP-VIIRS\nNighttime Light Composite Data at the County Level\nin China.” IEEE Journal of Selected Topics in\nApplied Earth Observations and Remote Sensing 8 (3): 1217–29. https://doi.org/10.1109/JSTARS.2015.2399416."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "",
    "text": "Welcome to my learning diaries\nHi, I am Hilman. Welcome to my portfolio. It’s my weekly learning diaries that hopefully can help me recalling what I have learn in CASA0023 Remotely Sensing Cities and Environments module"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "Introduction",
    "text": "Introduction\nShort introduction about me.\nI am Hilman Prakoso, an Indonesian living in London. Currently I am a Master of Science candidate in Urban Spatial Science Program in University College London. In my undergaduate, I studied architecture in a four-year program at Institute Technology Bandung.\nMy interest are related to socio-spatial analysis, spatial data visualisation, and informal settlement mapping. Along with 2 other friends, I built an urban data centre platrform for my hometown, Surabaya, called Suroboyo Ngalor Ngidul. This platform aims to collect and spread data about Surabaya, especially in urban mobilities, social, and economy."
  },
  {
    "objectID": "index.html#about-this-learning-diaries",
    "href": "index.html#about-this-learning-diaries",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "About this Learning Diaries",
    "text": "About this Learning Diaries\nEvery chapter discusses different topics which generally structured into 3 segments, summary, application, and reflection. Some topics will have different structure due to specific question to answer.\nRunning through this learning diaries will help me and you to understand what remote sensing is and has broad ideas of what we can do with earth observation in city planning"
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  week02 - Sensor Satellite Presentation",
    "section": "",
    "text": "In this week we learn about one of the sensor. But instead of having it in normal diary format, we want to try different format, Xaringan presentation.\n\n\n\n\n\n\n\n\nTo view this presentation full-screen, click here."
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  week03 - Remote Sensing Data",
    "section": "",
    "text": "4 Summary\nIn this week we learn about history of Landsat data and pre-processing of imagery satellite data.\nWe have to thank Virginia Wood for the landsat data we have nowadays because previously NASA wanted to use analogue system (RBV camera - TV camera with Green, Red and NIR spectrum) for their sensor. This is the spectral that catch by the Landsat RBV, and then cutted into only 4 bands, green, red, red-near IR, and near-IR. Then, Virginia Norwood suggested to use Multispectal Camera (MSS) which allows us to have 7 bands image. This became standar for the following landsat. Since then she is known as “the Mother of Landsat”\nPreprocessing step is a step we do to make our imagery data ready to be processed (classified) or analysed. It can be seen on the mind map that preprocessing steps we can do 3 thing, which are: 1. Correction 2. Data Joining 3. Enhancement Not every imagery data required all these 3 steps, it depends on the data we get and what we are going to do. Check this mind map to understand the outline of pre-processing steps.\nLearning about image enhancement in pre-processing step are really beneficial especially when we are dealing with huge datasets, such as hyperspectral and multi-temporal imagery (Rodarmel and Shan, 2002). Principal Component Analysis enhancement is a tools to reducing dimensionality if we are detecting land-use change from time to time using stacked multi-date data like what Deng (2008) in their paper. In detecting land-use change in Hangzhou City, China, they also use data from various sensors which are aerial photograph, SPOT-5 and Landsat-7. !\nThe image above shows one example of land use change from cropland to urban land. These image shows:\na ETM in 2000\nb aerial photograph in 2000\nc SPOT-5 in 2003\nd Ikonos in 2003\ne-h are the principal component.\nsource: Deng (2008) Using PCA wil make it easier to see detect the change because it will produce a new image (Principal component) that intensify the change (Ingebritsen and Lyon, 1985). In Deng’s paper, PCA was used to combining image band that taken from two different times into one new image. Changed area will have high correlation between two image, meanwhile unchanged are will have low correlation. Afterwards Deng classified and labelled the correlation value to detect changing area.\nIn this analysis the usage of PCA on multitemporal and multisensor data shows high accuracy value which is 89.54%. Other application of PCA in hyperspectral done by Rodarmel (2002), also shows satisfying result with 70% correct classification rate. Rodarmel and Shan, use hyperspectral data to detect component of land cover, such as, vegetation, mineral and soil type. They stacked band 1-5, 1-10 and 1-25 from HYDICE image and use PCA to generate 4 different PC images. Afterwards they classified the result of each PCA image and compared with original image PCA. The result shows PCA from band 1-5 is contain most information, while bands beyond 10 only contain noise.\nsource: Principal Component Analysis for Hyperspectral Image Classification\nAlthough, PCA helps reducing image dimensionality, the output does not give much information. It has to be processed using image classification, as shown in the 2 paper I discussed above (Licciardi et al., 2012). We can say that output of PCA is input for other analysis. Thus, to do a robust analysis, we also need to learn about methods other than image enhancement.\nThis week content about image correction, data joining and enhancement are really interesting because it is beneficial in practical and academical context. Imagery data that we get from sensor is not always perfect, it might have some error that come from the atmospher or the sensor itself. Especially in some region with high degree of moist, acquiring clear image is a challenge due to the existance of cloud (Deng et al., 2008). This can be tackled by atmospheric correction or if we use multitemporal data we can do PCA. Not only from external factors, but image error can also happend because the radiometric calibration of the sensor. Thus, to conduct an accurate and robust analysis we have to reduce the error by doing corrections.\nImage enhancement helps to handle huge data set from multi-temporal, multispectral, and multi-sensor images. This is beneficial to see the change of land use. For developing countries, where development happen organically, a lot of residential built without permission so the government does not have the data about land usage in the city. By using remote sensing to detect the land-use changing will help government to be aware of the informal residential and observe where informal development tend to happen over time.\nHowever, we cannot just stop at learning about image enhancement because it only give us input for further analysis. We should also learn about image classification to detect land use and land coverage. For classification methods you can go to chapter 6 and 7."
  },
  {
    "objectID": "week03.html#correction",
    "href": "week03.html#correction",
    "title": "3  week03 - Remote Sensing Data",
    "section": "4.1 Correction",
    "text": "4.1 Correction\n\nPre-processings are required (occasionally) because imagery data can contain flaws or errors from the sensor, atmosphere, terrain and more\nScan Line Correction (SLC) is pair of mirror to compansate the forward movement of satellite so the resulting scan are shown paralel\n\n\n\n\nGEE Interface\n\n\nFailed scan line landsat is on Landsat 7, because it moves in a zig zag, and the corrector made the image normal\nImagery was still distributed but it is hard to use with methods developed to estimates the gaps, termed gap filling. ### 1. Geometric Correction\n\nMeans geometry on how the image located on earth.\nSatelite image has CRS (Coordinate Reference System)\nImage distortions can be introduce due to:\n\nView angle (off-nadir)* - Nadir means directly down\n\n\n\n\nView Angle\n\n\n  - if the satelite off nadir, it will cause shadowing  and we have to correct it\n\nTopography (e.g. hills not flat ground)\nWind (if from a plane)\nRotation of the earth (from satellite)\n\nbecause of the earth rotation, the image produced will be off grid, so it has to be aligned\n\n\n\n\n\n4.1.0.1 Geometric Correction Solution\n\n\n\n\nGeometric correction solution\n\n\nSteps:\n\nidentify Ground Control Points (GCP) to match known points in the image and a reference dataset\n\nReference Dataset can be:\n\nLocal Map\nAnother image\nGPS data from handheld device\n\nGCP typically using object that does not move, such as:\n\nParking lot\nBuilding\n(typically non-vegetation)\n\n\nTake the coordinates and the model them to give geometric transformation coefficients\nTransform the GCP coordinates to the right one using linear regressrion\nplot these and try to minimise the RMSE\n\nModelling\n\nForward mapping\n\npredicting corrected image with uncorrected image\n\n\n\nForward Mapping formula\n\n\n x and y are positions in the corrected map\nBut the issue is that we are modelling the rectified x and y which could fall anywhere on the gold standard map (e.g. not on a grid square or at a floating point)\nforward mapping isn’t the most common one to use\n\n\n\nForward Mapping\n\n\n\nBackward Mapping\n\nPredicting the uncorrected image with the corrected image\n\n\n\nBackward Mapping\n\n\nEvery value in the output (corrected image) pixel will have value in the original input (uncorrected) image.\nthe images are distorted so might not completely overlap. The goal is to match distorted image with gold standard image, so we want the pixel to line up\n\n\n\nBackward Mapping\n\n\n\n\nResampling\n\nresampling is transforming from grid to another grid\nwe need to do re-sampling because the image data we get might slightly shifted. ANd is can be useful if the image has different grid size (or different band that has different resolution, like in the 1st week practical)\n\n\n\n\n\n4.1.1 2. Atmospheric Correction\n\nAtmospheric correction is the influence of atmospher on our data:\n\nAtmospheric scattering\ntopography Attenuation (reduction)\n\nThe goal is to remove the influence of atmosphere\nSituation where necessary or unnecessary to do atmospheric correction:\n\nUnnecessary\n\nif just look into one single images, because we dont have to see data across time\n\nNecessary\n\ntypically if we have time constrain. To compare data in multiple time stamp\n\n\nScattering create haze that reduce the contrast pf the image\nbright reflective material, eg, concrete, asphalt. karena terlalu terang jadi bocor ke sekitarnya\n\n\n4.1.1.1 Atmospheric Correction Solution\n\nRelative\n\nAdjust some data relative to something else as reference\nType\n\nDark object subtraction (DOS)\n\nDone by searching dark value (usually water) of each band and substract that value from each pixel\n\nPsuedo-invariant Features (PIFs)\n\nthis used when we have may images. We pick 1 image as based image. Determine feature that don’t change. Make regression model with Y is the based image. Adjust the ijmage based on regression model\n Pseudo invariant feature\n\n\n\nAbsolute\n\nChange digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet\nBasically made atmospheric model called atmospheric radiative transfer models\n\n\n\n\n\n4.1.2 3. Topography Correction/ Orthorectification Correction\n\nit means when we are not looking straight down (nadir), so the image distorted.\northorectification means removing distortion by making the pixel viewed at nadir\nfor orthorectification we need sun zenith and azimut angle, and orientation of the slope from DEM.\n\n\n\nZenith Azimut ilustration\n\n\n\nZenith meas directly up while nadir means directly down\nAzimut is position of sun to north, south, east, west\n\nAtmospheric typically happen before topographic correction\n\n\n\n4.1.3 4. Radiometric Calibration\n\nSensor capture image as Digital Number with no units. Spectral Radiance is the amount of light within a  band from a sensor in the field of view\nRadiometric Calibration: Calibrate the data (digital Number) into radiance and convert to specific unit\n\nRadiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor\n\nbasically, before sending sensor to the space, calibaration was done to check whether sensors performing correctly or not. We then use the calibration measurements to adjust the data captured by the sensor"
  },
  {
    "objectID": "week03.html#joining-datasetsenhancement",
    "href": "week03.html#joining-datasetsenhancement",
    "title": "3  week03 - Remote Sensing Data",
    "section": "4.2 Joining Datasets/Enhancement",
    "text": "4.2 Joining Datasets/Enhancement\n\nthe ideas is joining or merging or mosaicking or feathering images into one seamless image\nThis process was done by taking few pixel from each image at the same location and overlaping those pixels. Then, blending these to image on the ovelaped pixels\nThose image are ovelapping by 20-30%\n source Seamless Mosaic (l3harrisgeospatial.com)\n\n\nsource gdal - How to create a mosaic in QGIS with cutline and feathering for Landsat-8 imagery - Geographic Information Systems Stack Exchange - However the challenge is the image we get might coming from different day and lighting condition or even different satellite. It can cause different band value thus those image have to undergo standarisation and normalisation - Standarisation by dividing the SR value by a maximum value per band - normalisation by divide the standarised value by the sum of values across all bands ## Image Enhancement - it doesn’t alter the value of the data, merely changing how it explains and visual appearance ### Contrast Enhancement -  - done by: - stretching min max value - percentage Linear and Standar Deviation - Piecewise Linear Contrast Stretch - it doesn’t alter the value of the data, merely changing how it explains and visual appearance\n\n4.2.1 Ratio enhancement\n\nband ratioing means dividing bands by each other\neg: Normalize Burn Ratio\n\n\n\n\nratio enhancement\n\n\nsource: Landsat Normalized Burn Ratio | U.S. Geological Survey (usgs.gov)\n\n\n\n\n4.2.2 Filtering\n\nmeans taking an image an having a moving window to see aggregation of the image. Calculate surround pixel and put the average value as the middle pixel’ value\nLow Pass or low frequency is average of the surrounding pixels\nHigh pass or high frequency is enhance local vatiations\n ### Principal Component Analysis\nUsing PCA we can make our data smaller and maximise variation between our data\nPCA will transforming multi-spectral data into uncorrelated and smaller data set\nReduce dimensionality\nExample:\n\nmulti-temporal PCA bands from both time points are combined into one image, then PCA\n\n\n\nPCA\n\n\nso initially there are 2 or 3 images from different time stamps. It was stacked together and did PCA. From the PCA,they classify land use changes. Then maximise the variation.\n\n\n\n\n4.2.3 Texture Enhancement\n\nTextture is spatial variation of gray value\nusually used for medical detection\nTexture analysis looks at the tonal feature of the image by looking at the surrounding values. So there’s a moving window with 3x3 grid and it will calculate the value of a pixel with variance and probability of surrounding (within the window) value\nTexture variance result\n\n\n\n\nTexture Enhancement\n\n\nsisi yang terang adalah pinggir2 gedung karena tepi texture has high variance value\n\nTexture is beneficial to give additional information to our model as it oppose to just relying on spectral reflectance. Thus we can improve our classification model.\n\n\n\n4.2.4 Data Fusion enhancement\n\nstack of multiband data fused with PCA or texture or other enhancement.\nImage fusion can alse be from different sensor. eg. Sentinel fused with Landsat\nusually it take the median value of each pixel of each band."
  },
  {
    "objectID": "week04.html#summary-question-01",
    "href": "week04.html#summary-question-01",
    "title": "4  week04 - Policy",
    "section": "4.1 Summary (Question 01)",
    "text": "4.1 Summary (Question 01)\n\n4.1.1 Annual Natural Disaster in Semarang\nSemarang is the largest and capital city of Central Java. Due to its location next to Java sea, it becomes main port of Central Java and hold crucial role for food supply. In some area of Semarang, especially coastal area, drinking water is not accessible. Limited water resources forces residents to extract groundwater which cause land subsidence approximatelly 10cm per year. Thus make them suffer from tidal flooding every year. On the other hand, Semarang has fault geological structure that makes it prone to landslide (Faizana, Nugraha and Yuwono, 2015). Local government should anticipate the risk of natural disaster to ensures the continuity of the availability resources for local community (“City Resilience Index,” 2015).\n\n\n\nFlood in Semarang\n\n\nImage source: Tiga Orang Tewas Tersetrum Saat Banjir Semarang (cnnindonesia.com)\n\n\n\nSemarang Map\n\n\nImage source: Preliminary Identification of Urban Park Infrastructure Resilience in Semarang Central Java\n\n\n4.1.2 ARUP City Resilience Index\nArup has released City Resilience Index to help city government measure and monitor multiple significant factor to make a city resilience. The index are structured into 4 dimension which each of it are broken down into 3 goals with total 52 indicators.\n\n\n\nArup City Resilience Framework - Dimension, Goals, and Indicators\n\n\nUnder Infrastructure and Ecosystem dimension there are 3 goal which are reduced exposure and fragility (goals 7), effective provision of critical service (goal 8), and reliable mobility and communications (goals 9). There are some indicator that can be achieved by Semarang goverment using remote sensing data\n\n\n\nInfrastructure & Ecosystem Dimension\n\n\n\n\n\nGoals\n\n\nHere are further explanation of each indicators that can be tackled using remote sensing data\n7. REDUCED EXPOSURE & FRAGILITY\n7.1 Comprehensive hazard and exposure mapping\n\nRobust systems in place to map the city’s exposure and vulnerability to hazards based on current data.\n\n7.3 Effectively managed protective ecosystems\n\nWell-developed understanding and acknowledgement of the role of ecosystems in providing physical protection to the city.\n\n7.4 Robust protective infrastructure\n\nIntegrated, forward-looking and robust network of protective infrastructure that reduces vulnerability and exposure of citizens and critical assets.\n\n8. EFFECTIVE PROVISION OF CRITICAL SERVICES\n8.1 Effective stewardship of ecosystems\n\nRobust mechanisms are in place to maintain and enhance the ecosystem services that benefi t city residents.\n\n9. RELIABLE MOBILITY & COMMUNICATIONS\n9.2 Diverse and affordable transport networks\n\nDiverse and integrated transport networks, providing flexible and affordable travel around the city for all"
  },
  {
    "objectID": "week04.html#application-question-02",
    "href": "week04.html#application-question-02",
    "title": "4  week04 - Policy",
    "section": "4.2 Application (Question 02)",
    "text": "4.2 Application (Question 02)\nTo be resilience in natural disaster, a city needs comprehensive understanding of which are vulnerable to hazard (indicator 7.1). It can be done by mapping potential natural disaster based on previous flooding and clustering map. Making flooding map can be gained by regressing SAR flooding area data, rainfall data, sea level data (from satellite data), and topography data from Shuttle Radar Mission DEM (Lin et al., 2016). To make Landslide map we can comparing historic topography data from Shuttle Radar Mission DEM which movement of geological structure (Dewan et al., 2007). Combining these 2 maps, Semarang government will can have natural disaster risk map that can be guideline for city planning to focus on areas with high risk.\nNatural disaster can cause some residential areas be isolated. Thus, emergency access are required for massive evacuation and supply daily needs which also the indicator in goals number 9 in City Resilience Index. City government need to map crucial acces and alternative acces on disaster. In order to do that, mapping or clustered residential area are required which can be obtain from classifying landsat imagery data using NDBI. By assessing residential map dan natural disaster risk map, government can make crucial emergency transportation plan. It also can be enhance with agent-based modelling to simulate evacuation and basic needs supply. Hence, government can focusing their policy and city planning in building or improving emergency transportation access.\n\n\n\nApplication Workflow Ideas"
  },
  {
    "objectID": "week04.html#reflection-question-03",
    "href": "week04.html#reflection-question-03",
    "title": "4  week04 - Policy",
    "section": "4.3 Reflection (question 03)",
    "text": "4.3 Reflection (question 03)\nInternational policy help local government, that sometimes does not have a robust policy, to build and develop a resilience city. In order to achieve International policy’s target, earth observation data can be a helpful resources for decision making and city planning. Satellite imagery data allows government to have macro and micro scale observation of their historic or existing city condition. It can be beneficial to detect existing urban fabrics and potential urban network. Imagery data are also beneficial to record pre-during-post condition of an event or disaster. Thus in the future local government can prevent or protect their citizen better especially for city with periodic disaster like Semarang or Greater Dhaka (Dewan et al., 2007). Remote Sensing data allows Semarang government to achieve not only one but five indicators of ARUP City Resilience Index.\n\n\n\n\n“City Resilience Index” (2015). ARUP.\n\n\nDewan, A. M. et al. (2007) “Evaluating Flood Hazard for Land-Use Planning in Greater Dhaka of Bangladesh Using Remote Sensing and GIS Techniques,” Water Resources Management, 21(9), pp. 1601–1612. doi: 10.1007/s11269-006-9116-1.\n\n\nFaizana, F., Nugraha, A. L. and Yuwono, B. D. (2015) “Jurnal Geodesi Undip,” 4.\n\n\nLin, L. et al. (2016) “A review of remote sensing in flood assessment,” in 2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics). 2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics), pp. 1–4. doi: 10.1109/Agro-Geoinformatics.2016.7577655."
  },
  {
    "objectID": "week05.html#google-earth-engine-introduction",
    "href": "week05.html#google-earth-engine-introduction",
    "title": "5  week_05",
    "section": "6.1 Google Earth Engine Introduction",
    "text": "6.1 Google Earth Engine Introduction\nGoogle Earth Engine (GEE) is a geospatial processing server developed by Google. It allows geospatial analysis with massive raster or vector dataset that has Landsat and sentinel dataset from time to time. It allows to handle massive data store with a fast respons because all the data are stored on the server. GEE has client side and server side. Client side is what user see or frontend, meanwhile server side is the backend where GEE store all the data as Earth Engine object or identify as ee. In the GEE script, if we found anything with ee it means that it is stored in the server. Client side is place where GEE collect input from the user and after that the input will be processed in server side."
  },
  {
    "objectID": "week05.html#google-earth-engine-interface",
    "href": "week05.html#google-earth-engine-interface",
    "title": "5  week_05",
    "section": "6.2 Google Earth Engine Interface",
    "text": "6.2 Google Earth Engine Interface\nInterface of Google Earth Engine will look like this: [[Pasted image 20230318152345.png]] - Assets is where we upload our data- Documentation is documantation - Code editor is where we put our script - Tools is tools helping navigating maps - inspector quiery the map itself and get some information"
  },
  {
    "objectID": "week05.html#how-google-earth-engine-works",
    "href": "week05.html#how-google-earth-engine-works",
    "title": "5  week_05",
    "section": "6.3 How Google Earth Engine works",
    "text": "6.3 How Google Earth Engine works\nBefore we go further to talk about any processes we can do in Google Earth Engine, it’s better to know how GEE works. Unlike previous week and term when we use R and python, GEE is using javascript. Thus, we can not do looping like what we usually do, instead in javascript we use mapping. Mapping is making function and save it into an variable (or object) and applied to the entire collection. But, what is variable and object? in Javascript variables are container of data value written as var. Object is also variables but can contain many data values. Go check w3schools explanation on object and variable.\nIn GEE, there are 9 types or classes of objects which are shown in the image below: [[Pasted image 20230319003026.png]] source: Objects in Google Earth Engine\nIt’s good to be familiar with object terminology in GEE because in the next section and next week we will use these terminology a lot. individual Landsat or satellite scene, which is a raster, is known as Image. - Image is raster data or individual satellite imagery scene - Geometry is vector or we know it as polygon - Feature is vector data with attributes (table or metadata) - Collection is stack data means that we have more than 1 (plural) data. So collection can be: - FeatureCollection which is stack or collection of vector data with attribute - ImageCollection which is stack or collection of raster image data\nThese 9 class of object also cover data manipulation which are: - Array is for multi-dimensional analysis - join is to join 2 or more dataset, and it also can be used to join imagery collection together from different sensor (such as: Landsat and sentinel) into one massive stack - Reduce is taking lots of data andsummarise into 1 point (it similar to filter or texture measure) - Reducer we will discuss this in the next section\nEach class of object has specific GEE function. The function is name of the object class with additional ee. in front of it, so for example it will look like this: - for Image use ee.Image - for image collection use ee.ImageCollection\nIn GEE, we do not need to worry about projection because by default it will be set to EPSG 3857. Only if needed (eg: if we want to load in R) we can change the projection at the end.\nAfter we understand brief information, interface and how GEE works, we can discuss further about one of methods or process that can be done in GEE. This process is Image Reducer. We will find this process beneficial because this methods helps reducing massive datasets that we are often dealt with in GEE analysis."
  },
  {
    "objectID": "week05.html#reducing-images",
    "href": "week05.html#reducing-images",
    "title": "5  week_05",
    "section": "6.4 Reducing Images",
    "text": "6.4 Reducing Images\nWorking with GEE allow us to work with multi-sensor and multi temporal data. It makes us tempted to work with with massive collection of imagery data. GEE develop an image reduction tool to summarise collection of image into 1 image composite using ee.Reducer\nThere are some reduction methods we can do which are: 1. Reduce with median 2. Reduce by region 3. Reduce by Neighbourhood 4. Reduce with Linear Regression\nWe will discuss this methods one by one\n\n6.4.1 Reduce by Median\nThe case to use this technic is if we have lots of collection images but we only want to classify 1 image composite. Typically what GEE will do is take the median value of each pixel to make an Image composite. For instance, we want to analyse a study area in january 2023, we have image collection from several dates in January. GEE will check value of each pixel from different date and find the middle value. Afterwards, GEE will compose a new image from those median value. The median value of composed image might come from different date. [[Screenshot 2023-03-19 at 12.32.24 (1).png]] Although this methods commonly used (even by the lecturer, Andy), (Citation Hansen) argue to this methods of image reduction because if we only take median value it is possible to loss a lot of information. Thus, instead of using meding, he suggest to use decile.\n\n\n6.4.2 Reduce by Region\nOther technic of reducing image is by region. Reducing image by region can be beneficial if we have massive dataset and want to do statistic analysis on one specific study area. For instance, we want to calculate average reflectance for each band within London (the study area). What we can do is taking polygon (or using .shp file) boundaries of a region, in this case London. A region boundary possibly has a lot of pixel within. Afterwards, GEE will reduce the image by calculating mean value of each pixel within the region boundary.\nReduce by region can be done for one study area or many study area (many polygon boundaries) using: - image.reduceRegion() for 1 study polygon - image.reduceRegions() for more than 1 study polygon\n\n\n6.4.3 Reduce by Neigbourhood\nInstead of using region boundary, we can use Kernel neighbourhood to reduce the image. In this reducing methods is kind of similar to filtering and texture enhancement that we learnt in Chapter 3. Basically, the methods is we ask GEE to summarize the image by calculating either mean, median, min, or max value of the neighbour. Neighbourhood means, window pixels surround the central pixel. Thus, to get a value of a pixel, GEE will calculate window pixels surround it like shown in the image below. The function for reducing by neighbourhood is .reduceNeighbourhood() [[Pasted image 20230318174849.png]]\nGEE will compose a new image from the calculation result. The Image on the left is the previous satellite image and image on the right is the composite image after reducing by neighbourhood. [[Pasted image 20230318175307.png]] source: GEE |  Google Earth Engine  |  Google Developers\n\n\n6.4.4 Reduce using Linear Regression\nBiggest benefit of GEE is we can look out data from different time so we can see how pixel values change overtime. in GEE, we can do single linear regression by using linearFit() function if we want to see the change overtime in pixel values. the function takes 2 bands which is one for dependent variable and the other is the independent variable. The observation is often time. For example, we have an image collection in January and going to see temperature change overtime. The GEE will run regression pixel by pixel with time and give us output value of the regression slope. The continues values of the slope will be mapped into new image composite. therefore, this regression technic also considered as a reduce image methods because we are reducing all the stacked data into an images. The image below is the result of linear regression. The blue color show positive slope which means the band value increase overtime. Meanwhile, the red colour show negative slope that means the value decrease overtime. [[Pasted image 20230318195410.png]]\nOther then single linear regression, GEE can also do do Multivariate Multiple Linear Regression (MMLR) that using multiple dependent variables and independent variables. Multivariate Multiple Linear Regression is a statistical test used for more than one outcome variables using one or more independent variables (Citation Multivariate Multiple Linear Regression). The methods is similar to single linear regression that it will run pixel by pixel but for all dependent variables. The only difference is MMLR will calculate covariance matrix.\nThe case using MMLR in GEE is usually we want to calculate 2 bands for dependent variable (outcome variables). For example, we want to model pecipitation and temperature with constant variable (or control variable) and time. Each dependent variable doing its own regression pixel by pixel then it will calculate covariance matrix. Covariance is to find the value that indicates how these two variables vary together. Covariance is like variance but using 2 variables. Covariance matrix represent covariance values of each pair of variables in multivariate data (citation: 5 Things You Should Know About Covariance).\nThat was all the summary of this week material about Google Earth Engine and one of the process we can do in GEE, reduce image. There are benefits and drawbacks from using this methods. To understand better about this concept, we will see some application of this process."
  },
  {
    "objectID": "week05.html#spatio-temporal-changes-of-lake-surface-water-temperature",
    "href": "week05.html#spatio-temporal-changes-of-lake-surface-water-temperature",
    "title": "5  week_05",
    "section": "7.1 Spatio-temporal Changes of Lake Surface Water Temperature",
    "text": "7.1 Spatio-temporal Changes of Lake Surface Water Temperature\nCloud-based storage system of GEE allow us to do massive analysis with hundreds of images. This can be beneficial if we want to do study about spatio temporal changes because in GEE we can access data from multiple sensor and temporal. (albarqouniAssessmentSpatioTemporalChanges2022?) use GEE to analysis long-term spatio-temporal changes of lake surface water temperature (LSWT). To see change, this study includes 606 images from 2000 to 2021 from Landsat 5 TM and Landsat 8 OLI. 2 image sources were needed because Landsat 5 TM only collect data until 2011; thus we need Landsat 8 OLI to acquire data in 2013-2021. To calculate Water Surface Area Extraction, multispectral bands of these image collections was applied to normalized difference water index (NDWI). To measure LSWT, thermal bands was utilised to calculate Land Surface Emissivity. Then, relationship of LSWT and water surface area will be assessed. Since this analysis was conducted in GEE, the image collection was stored in the cloud. [[Pasted image 20230324044812.png]] This is the analysis flowchart. source: (albarqouniAssessmentSpatioTemporalChanges2022?)"
  },
  {
    "objectID": "week05.html#forest-canopy-closure",
    "href": "week05.html#forest-canopy-closure",
    "title": "5  week_05",
    "section": "7.2 Forest Canopy Closure",
    "text": "7.2 Forest Canopy Closure\n(xieImprovedForestCanopy2022?) takes advantages of GEE to monitoring forest changes. This study determining vegetation and bare soil endmember the normalized differences vegetation index (NDVI), modified bare soil index (MBSI), and bare soil index (BSI). This study also utilising GEE’s abilities of conducting massive analysis and providing data from various satellite. To help handle big dataset, this studyproduce image composite by reducing topographic-corrected cloudless using median reducer to pick median value in each band, in each pixel, overtime. Using image composite helps the analysis process becase it summarise the image collection into 1 image composite. This study tested the proposed methods using Landsat 8 and Sentinel-2 data. The result show that the proposed method is robust and replicatable using different sensor but with similar wavebands. To help handling.\n[[Pasted image 20230324053931.png]] source: (albarqouniAssessmentSpatioTemporalChanges2022?)\nWhat we can learn from this application is, various satellite data given by GEE can be combined into one analysis like what we see in LSWT research. Also, it can be treated as different input to compare the model like what we see in Forest Canopy Closure. in GEE it will be easy to access data from different tima and sensor. Although we can get image collection easily, we have to be critical to image that we get. For example, do our images require atmospheric correction? Especially regarding this example, estevezGaussianProcessesRetrieval2022, succesfully avoided atmospheric correction by assessing top-of-atmspher (TOA)"
  },
  {
    "objectID": "week05.html#unsupervised-learning",
    "href": "week05.html#unsupervised-learning",
    "title": "5  week06 - Google Earth Engine",
    "section": "6.1 Unsupervised Learning",
    "text": "6.1 Unsupervised Learning\nUnsupervised Learning is using machine to analyse or cluster our data. So what usually will happen is we give our data, then we ask it to be cluster the dat into certain number of cluster. The machine will return with cluster of data ask we requested. In the context of imagery satellite data, the given data is value of band for each pixel.\nUnsupervised learning usually refered to clustering/k-means. In clustering, we have spectral feature space by plotting 2 or more bands value against each other. If we only have 2 bands values, the pixel value will be plotted in 2 dimension feature space. As we add more band value, the dimension will be added as well. After we give our data, we have to set some rules to tell the machine how the data should be clustered. The machine then will start random center point of cluster with radius that has been set. Pixels value within same radius are in the same cluster. machine will start moving it’s center point until all pixels has been alocated to a cluster.\nDetail steps of clustering: 1. Give data 2. Then set series of rules, such as: - the radius in spectral feature to define where new cluster started - spectral distance measure - number of pixels to be considered before merging - max number of cluster 3. It will repeat until no pixel alocated again [[Pasted image 20230321014355.png]] source: citation Evolutionary Computation Theory for Remote Sensing Image Clustering: A Survey\nOther methods of unsuervised learning is ISODATA. This methods is specifically for earth observation data. This methods is similar to K-means clustering but with some additional rules, which are: - Any clusters have so few pixels = meaningless - Clusters are so close they can be merged - Clusters can be split - elongated clusters in feature space\nAlthough learning using ISODATA is beneficial because it’s sepcifically used for EO data, this methods has a drawback which is can create a lots of clusters and difficult to assign meaning. For example it can cause two types of landcover in a pixel. To tackle this drawbacks we can do Cluster busting which is basically mask cluster that incorrect and label to new one. [[Pasted image 20230321162157.png]]"
  },
  {
    "objectID": "week05.html#supervised-learning",
    "href": "week05.html#supervised-learning",
    "title": "5  week06 - Google Earth Engine",
    "section": "6.2 Supervised Learning",
    "text": "6.2 Supervised Learning\nDifferent from unsupervised learning, supervised learning uses labelled training data. Training data is dataset that being used to train the model, so the model can classify the data for us. Other than training data, we have testing data. Testing data is dataset that being used to assess our model. Training and testing data has to be mutually exclusive.\nGenerally steps of supervised learning are: 1. Class Definition 2. Pre-processing 3. Training 4. Pixel Assignment 5. Accuracy assignment\nMethods on supervised learning are categorized into 2: 1. Parametric - parametric use data that are normally distributed - Methods in parametric: - Maximum likelihood 1. Non-parametric - Non-parametric use data that are not normally distributed - Methods in parametric: - Density slicing - Parallelpiped - Minimum distance to mean - Nearest neighbor - Neural networks - Machine learning / expert systems* (eg. Support Vector Machine, Neural Networks)\nIt is quiet rare to have normally distributed data in earth observation image. Moreover, recently most works uses machine learning/expert systems or spectral mixture analysis."
  },
  {
    "objectID": "week05.html#be-aware",
    "href": "week05.html#be-aware",
    "title": "5  week06 - Google Earth Engine",
    "section": "6.3 Be Aware",
    "text": "6.3 Be Aware\nEven though, image classifiction will be done by machine, as researcher or the one who assign the machine we have to be aware of several things:\n\n6.3.1 Hard Classification or Fuzzy Classification?\nFirst we have to be aware, are we going to do hard classification or fuzzy classification. Hard classification means there will be a define catergory like water or land in our image. Fuzzy classification means we have to classify into fuzzy continuous value. [[Pasted image 20230321171655.png]]\n\n\n6.3.2 How’s the pixel content?\nsecond, we have to be careful whether we are going to see sub pixel value, whole general pixel value, or some pixel value. It is important te be a concern because it’s quiet rare to have a completely homogenous pixel. Sometimes we have to find the majority value of the pixel. [[Pasted image 20230321171932.png]] source: citation # The pixel: A snare and a delusion\n\n\n6.3.3 Pixel or Object?\nLast, we have to consider are we going to look at pixel or object on the image data. Example of pixel are shon in image on the left, while example of object shown on the right image. It is important because our data observation will be different between pixel or object. [[Pasted image 20230321195605.png]] source: Superpixels supercells\nIt is important to define this constraint before we do classificatio because we are the one who will select the propriate methods and ask the machine.\nAfter considering those constrain we should select the propriate methods for our classification. Working with machine, make us temped to work with complicated model. However, not every condition are required complicated methods. Complicated methods might have higher accuracy but it will be harder to interprate. I present this illustration to show the trade off between accuracy and interpretability.\n[[Pasted image 20230321200314.png]] source: citation Support Vector Machine vs. Random Forest for Remote Sensing Image Classification: A Meta-analysis and systematic review\nPlease choose classification methods wisely."
  },
  {
    "objectID": "week05.html#natural-disaster-vulnerability-map-using-support-vector-machine-svm",
    "href": "week05.html#natural-disaster-vulnerability-map-using-support-vector-machine-svm",
    "title": "5  week06 - Google Earth Engine",
    "section": "7.1 Natural Disaster Vulnerability map using Support Vector Machine (SVM)",
    "text": "7.1 Natural Disaster Vulnerability map using Support Vector Machine (SVM)\n\n7.1.1 Flood\nNatural disaster can cause devastating impact on economy, social, and environment. Flood is one of the most destructive disaster that happen frequently in some areas in the world. It force local community and government to manage the disaster and be resilience towards it. Therefore, recognizing flood-prone areas is essential initial step in planning flood management and resilience strategy. Flood is a dynamic and complex disaster that is caused by a lot of factors. To assessing flood vlunerable area, we have to put many things into account such as, rainfall data, land cover, topography, and so on. One of supervised learning’s benefit is effective in high dimension (SupportVectorMachines?). (youssefFloodVulnerabilityMapping2023a?) used this advantage to assess flood phenomena in Taif in Saudi Arabia. The study showed combination 13 parameters, including remote sensing data, and Support Vector Machine (SVM), can make a vulnerability maps like shown in the image below. [[Pasted image 20230324010018.png]] This vulnerability map can be used by policy maker to pointed action plan to prevent and mitigate flooding. Different class in this vulnerabiliity map guides planner to understand which area are highly effected by flood.\nOther than SVM, this study has tried to use bivariate and multivariate methods. Apparently, SVM model performed best with 96.2% accuracy. Earllier, (mojaddadiEnsembleMachinelearningbasedGeospatial2017?) made flood vulnerability map using SVM as well for Damansara, Malaysia. The result also indicates high classification accuracy with 84.07%. Although in the summary SVM were shown has low interpretability, this study show that using the right data and combination of methods we can get high accuracy and easy to understand result.\n\n\n7.1.2 Drought\nnot only for flood, supervised classification also had been used to assess other natural disaster. (ghasempourDroughtVulnerabilityAssessment2022?) made Drought vulnerability maps northwest part of Iran. The study used SVM on 17 geo-environmental parameters including temperature and soil condition which are not included by (mojaddadiEnsembleMachinelearningbasedGeospatial2017?) and (youssefFloodVulnerabilityMapping2023a?) in their researches.\n[[Pasted image 20230324014809.png]] Drought Vulnerability Map of Northwest Iran source: (ghasempourDroughtVulnerabilityAssessment2022?)"
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "6  week_06",
    "section": "",
    "text": "7 Summary\nIn this week we learn about Classification, this week topic is related to module CASA0006 Data Science for Spatial System. As we know remote sensing collect imagery satellite data, but what we can do to identify information in the imagery data? how to identify what happening in the image? What is land use and land cover of the given image? is it forest are or urban are?\nWe can identify the information using Image classification methods. The image classifiation is done computationally. If we see an imagery data we can identify or classify the object within the image by judging based on our experience or called Inductive Learning. Using that idea, now we train the computer system to have human knowledge to solve problem called Machine Learning. We can train computer system to learn on how to identifying satellite imagery data using classification methods.\nin Statistic, classification ideas is basically assigning our data into certain categories. Image Classification is basically turn every pixel on the image into one categorical classification. for example our pixel can be identified as forest or residential, high or low level forest combustion and so on. There are 2 types of image classification which are Supervised and unsupervised. Image classification has been being used since 1970. It was started with unsupervised in 1970s, to supervised classification in 1975s and now mostly we have been using Object-Based Image Analysis since 1995s. But what is supervised and unsupervised classification? Check this table for general idea of those 2 classification. But I will tell futher in the explanation later.\nNow, we will discuss one by one."
  },
  {
    "objectID": "week06.html#google-earth-engine-introduction",
    "href": "week06.html#google-earth-engine-introduction",
    "title": "6  week05",
    "section": "7.1 Google Earth Engine Introduction",
    "text": "7.1 Google Earth Engine Introduction\nGoogle Earth Engine (GEE) is a geospatial processing server developed by Google. It allows geospatial analysis with massive raster or vector dataset that has Landsat and sentinel dataset from time to time. It allows to handle massive data store with a fast respons because all the data are stored on the server. GEE has client side and server side. Client side is what user see or frontend, meanwhile server side is the backend where GEE store all the data as Earth Engine object or identify as ee. In the GEE script, if we found anything with ee it means that it is stored in the server. Client side is place where GEE collect input from the user and after that the input will be processed in server side."
  },
  {
    "objectID": "week06.html#google-earth-engine-interface",
    "href": "week06.html#google-earth-engine-interface",
    "title": "6  week05",
    "section": "7.2 Google Earth Engine Interface",
    "text": "7.2 Google Earth Engine Interface\nInterface of Google Earth Engine will look like this: [[Pasted image 20230318152345.png]] - Assets is where we upload our data- Documentation is documantation - Code editor is where we put our script - Tools is tools helping navigating maps - inspector quiery the map itself and get some information"
  },
  {
    "objectID": "week06.html#how-google-earth-engine-works",
    "href": "week06.html#how-google-earth-engine-works",
    "title": "6  week05",
    "section": "7.3 How Google Earth Engine works",
    "text": "7.3 How Google Earth Engine works\nBefore we go further to talk about any processes we can do in Google Earth Engine, it’s better to know how GEE works. Unlike previous week and term when we use R and python, GEE is using javascript. Thus, we can not do looping like what we usually do, instead in javascript we use mapping. Mapping is making function and save it into an variable (or object) and applied to the entire collection. But, what is variable and object? in Javascript variables are container of data value written as var. Object is also variables but can contain many data values. Go check w3schools explanation on object and variable.\nIn GEE, there are 9 types or classes of objects which are shown in the image below: [[Pasted image 20230319003026.png]] source: Objects in Google Earth Engine\nIt’s good to be familiar with object terminology in GEE because in the next section and next week we will use these terminology a lot. individual Landsat or satellite scene, which is a raster, is known as Image. - Image is raster data or individual satellite imagery scene - Geometry is vector or we know it as polygon - Feature is vector data with attributes (table or metadata) - Collection is stack data means that we have more than 1 (plural) data. So collection can be: - FeatureCollection which is stack or collection of vector data with attribute - ImageCollection which is stack or collection of raster image data\nThese 9 class of object also cover data manipulation which are: - Array is for multi-dimensional analysis - join is to join 2 or more dataset, and it also can be used to join imagery collection together from different sensor (such as: Landsat and sentinel) into one massive stack - Reduce is taking lots of data andsummarise into 1 point (it similar to filter or texture measure) - Reducer we will discuss this in the next section\nEach class of object has specific GEE function. The function is name of the object class with additional ee. in front of it, so for example it will look like this: - for Image use ee.Image - for image collection use ee.ImageCollection\nIn GEE, we do not need to worry about projection because by default it will be set to EPSG 3857. Only if needed (eg: if we want to load in R) we can change the projection at the end.\nAfter we understand brief information, interface and how GEE works, we can discuss further about one of methods or process that can be done in GEE. This process is Image Reducer. We will find this process beneficial because this methods helps reducing massive datasets that we are often dealt with in GEE analysis."
  },
  {
    "objectID": "week06.html#reducing-images",
    "href": "week06.html#reducing-images",
    "title": "6  week05",
    "section": "7.4 Reducing Images",
    "text": "7.4 Reducing Images\nWorking with GEE allow us to work with multi-sensor and multi temporal data. It makes us tempted to work with with massive collection of imagery data. GEE develop an image reduction tool to summarise collection of image into 1 image composite using ee.Reducer\nThere are some reduction methods we can do which are: 1. Reduce with median 2. Reduce by region 3. Reduce by Neighbourhood 4. Reduce with Linear Regression\nWe will discuss this methods one by one\n\n7.4.1 Reduce by Median\nThe case to use this technic is if we have lots of collection images but we only want to classify 1 image composite. Typically what GEE will do is take the median value of each pixel to make an Image composite. For instance, we want to analyse a study area in january 2023, we have image collection from several dates in January. GEE will check value of each pixel from different date and find the middle value. Afterwards, GEE will compose a new image from those median value. The median value of composed image might come from different date. [[Screenshot 2023-03-19 at 12.32.24 (1).png]] Although this methods commonly used (even by the lecturer, Andy), (Citation Hansen) argue to this methods of image reduction because if we only take median value it is possible to loss a lot of information. Thus, instead of using meding, he suggest to use decile.\n\n\n7.4.2 Reduce by Region\nOther technic of reducing image is by region. Reducing image by region can be beneficial if we have massive dataset and want to do statistic analysis on one specific study area. For instance, we want to calculate average reflectance for each band within London (the study area). What we can do is taking polygon (or using .shp file) boundaries of a region, in this case London. A region boundary possibly has a lot of pixel within. Afterwards, GEE will reduce the image by calculating mean value of each pixel within the region boundary.\nReduce by region can be done for one study area or many study area (many polygon boundaries) using: - image.reduceRegion() for 1 study polygon - image.reduceRegions() for more than 1 study polygon\n\n\n7.4.3 Reduce by Neigbourhood\nInstead of using region boundary, we can use Kernel neighbourhood to reduce the image. In this reducing methods is kind of similar to filtering and texture enhancement that we learnt in Chapter 3. Basically, the methods is we ask GEE to summarize the image by calculating either mean, median, min, or max value of the neighbour. Neighbourhood means, window pixels surround the central pixel. Thus, to get a value of a pixel, GEE will calculate window pixels surround it like shown in the image below. The function for reducing by neighbourhood is .reduceNeighbourhood() [[Pasted image 20230318174849.png]]\nGEE will compose a new image from the calculation result. The Image on the left is the previous satellite image and image on the right is the composite image after reducing by neighbourhood. [[Pasted image 20230318175307.png]] source: GEE |  Google Earth Engine  |  Google Developers\n\n\n7.4.4 Reduce using Linear Regression\nBiggest benefit of GEE is we can look out data from different time so we can see how pixel values change overtime. in GEE, we can do single linear regression by using linearFit() function if we want to see the change overtime in pixel values. the function takes 2 bands which is one for dependent variable and the other is the independent variable. The observation is often time. For example, we have an image collection in January and going to see temperature change overtime. The GEE will run regression pixel by pixel with time and give us output value of the regression slope. The continues values of the slope will be mapped into new image composite. therefore, this regression technic also considered as a reduce image methods because we are reducing all the stacked data into an images. The image below is the result of linear regression. The blue color show positive slope which means the band value increase overtime. Meanwhile, the red colour show negative slope that means the value decrease overtime. [[Pasted image 20230318195410.png]]\nOther then single linear regression, GEE can also do do Multivariate Multiple Linear Regression (MMLR) that using multiple dependent variables and independent variables. Multivariate Multiple Linear Regression is a statistical test used for more than one outcome variables using one or more independent variables (Citation Multivariate Multiple Linear Regression). The methods is similar to single linear regression that it will run pixel by pixel but for all dependent variables. The only difference is MMLR will calculate covariance matrix.\nThe case using MMLR in GEE is usually we want to calculate 2 bands for dependent variable (outcome variables). For example, we want to model pecipitation and temperature with constant variable (or control variable) and time. Each dependent variable doing its own regression pixel by pixel then it will calculate covariance matrix. Covariance is to find the value that indicates how these two variables vary together. Covariance is like variance but using 2 variables. Covariance matrix represent covariance values of each pair of variables in multivariate data (citation: 5 Things You Should Know About Covariance).\nThat was all the summary of this week material about Google Earth Engine and one of the process we can do in GEE, reduce image. There are benefits and drawbacks from using this methods. To understand better about this concept, we will see some application of this process."
  },
  {
    "objectID": "week06.html#spatio-temporal-changes-of-lake-surface-water-temperature",
    "href": "week06.html#spatio-temporal-changes-of-lake-surface-water-temperature",
    "title": "6  week05",
    "section": "8.1 Spatio-temporal Changes of Lake Surface Water Temperature",
    "text": "8.1 Spatio-temporal Changes of Lake Surface Water Temperature\nCloud-based storage system of GEE allow us to do massive analysis with hundreds of images. This can be beneficial if we want to do study about spatio temporal changes because in GEE we can access data from multiple sensor and temporal. (albarqouniAssessmentSpatioTemporalChanges2022?) use GEE to analysis long-term spatio-temporal changes of lake surface water temperature (LSWT). To see change, this study includes 606 images from 2000 to 2021 from Landsat 5 TM and Landsat 8 OLI. 2 image sources were needed because Landsat 5 TM only collect data until 2011; thus we need Landsat 8 OLI to acquire data in 2013-2021. To calculate Water Surface Area Extraction, multispectral bands of these image collections was applied to normalized difference water index (NDWI). To measure LSWT, thermal bands was utilised to calculate Land Surface Emissivity. Then, relationship of LSWT and water surface area will be assessed. Since this analysis was conducted in GEE, the image collection was stored in the cloud. [[Pasted image 20230324044812.png]] This is the analysis flowchart. source: (albarqouniAssessmentSpatioTemporalChanges2022?)"
  },
  {
    "objectID": "week06.html#forest-canopy-closure",
    "href": "week06.html#forest-canopy-closure",
    "title": "6  week05",
    "section": "8.2 Forest Canopy Closure",
    "text": "8.2 Forest Canopy Closure\n(xieImprovedForestCanopy2022?) takes advantages of GEE to monitoring forest changes. This study determining vegetation and bare soil endmember the normalized differences vegetation index (NDVI), modified bare soil index (MBSI), and bare soil index (BSI). This study also utilising GEE’s abilities of conducting massive analysis and providing data from various satellite. To help handle big dataset, this studyproduce image composite by reducing topographic-corrected cloudless using median reducer to pick median value in each band, in each pixel, overtime. Using image composite helps the analysis process becase it summarise the image collection into 1 image composite. This study tested the proposed methods using Landsat 8 and Sentinel-2 data. The result show that the proposed method is robust and replicatable using different sensor but with similar wavebands. To help handling.\n[[Pasted image 20230324053931.png]] source: (albarqouniAssessmentSpatioTemporalChanges2022?)\nWhat we can learn from this application is, various satellite data given by GEE can be combined into one analysis like what we see in LSWT research. Also, it can be treated as different input to compare the model like what we see in Forest Canopy Closure. in GEE it will be easy to access data from different tima and sensor. Although we can get image collection easily, we have to be critical to image that we get. For example, do our images require atmospheric correction? Especially regarding this example, estevezGaussianProcessesRetrieval2022, succesfully avoided atmospheric correction by assessing top-of-atmspher (TOA)"
  },
  {
    "objectID": "week06.html#unsupervised-learning",
    "href": "week06.html#unsupervised-learning",
    "title": "6  week_06",
    "section": "7.1 Unsupervised Learning",
    "text": "7.1 Unsupervised Learning\nUnsupervised Learning is using machine to analyse or cluster our data. So what usually will happen is we give our data, then we ask it to be cluster the dat into certain number of cluster. The machine will return with cluster of data ask we requested. In the context of imagery satellite data, the given data is value of band for each pixel.\nUnsupervised learning usually refered to clustering/k-means. In clustering, we have spectral feature space by plotting 2 or more bands value against each other. If we only have 2 bands values, the pixel value will be plotted in 2 dimension feature space. As we add more band value, the dimension will be added as well. After we give our data, we have to set some rules to tell the machine how the data should be clustered. The machine then will start random center point of cluster with radius that has been set. Pixels value within same radius are in the same cluster. machine will start moving it’s center point until all pixels has been alocated to a cluster.\nDetail steps of clustering: 1. Give data 2. Then set series of rules, such as: - the radius in spectral feature to define where new cluster started - spectral distance measure - number of pixels to be considered before merging - max number of cluster 3. It will repeat until no pixel alocated again [[Pasted image 20230321014355.png]] source: citation Evolutionary Computation Theory for Remote Sensing Image Clustering: A Survey\nOther methods of unsuervised learning is ISODATA. This methods is specifically for earth observation data. This methods is similar to K-means clustering but with some additional rules, which are: - Any clusters have so few pixels = meaningless - Clusters are so close they can be merged - Clusters can be split - elongated clusters in feature space\nAlthough learning using ISODATA is beneficial because it’s sepcifically used for EO data, this methods has a drawback which is can create a lots of clusters and difficult to assign meaning. For example it can cause two types of landcover in a pixel. To tackle this drawbacks we can do Cluster busting which is basically mask cluster that incorrect and label to new one. [[Pasted image 20230321162157.png]]"
  },
  {
    "objectID": "week06.html#supervised-learning",
    "href": "week06.html#supervised-learning",
    "title": "6  week_06",
    "section": "7.2 Supervised Learning",
    "text": "7.2 Supervised Learning\nDifferent from unsupervised learning, supervised learning uses labelled training data. Training data is dataset that being used to train the model, so the model can classify the data for us. Other than training data, we have testing data. Testing data is dataset that being used to assess our model. Training and testing data has to be mutually exclusive.\nGenerally steps of supervised learning are: 1. Class Definition 2. Pre-processing 3. Training 4. Pixel Assignment 5. Accuracy assignment\nMethods on supervised learning are categorized into 2: 1. Parametric - parametric use data that are normally distributed - Methods in parametric: - Maximum likelihood 1. Non-parametric - Non-parametric use data that are not normally distributed - Methods in parametric: - Density slicing - Parallelpiped - Minimum distance to mean - Nearest neighbor - Neural networks - Machine learning / expert systems* (eg. Support Vector Machine, Neural Networks)\nIt is quiet rare to have normally distributed data in earth observation image. Moreover, recently most works uses machine learning/expert systems or spectral mixture analysis."
  },
  {
    "objectID": "week06.html#be-aware",
    "href": "week06.html#be-aware",
    "title": "6  week_06",
    "section": "7.3 Be Aware",
    "text": "7.3 Be Aware\nEven though, image classifiction will be done by machine, as researcher or the one who assign the machine we have to be aware of several things:\n\n7.3.1 Hard Classification or Fuzzy Classification?\nFirst we have to be aware, are we going to do hard classification or fuzzy classification. Hard classification means there will be a define catergory like water or land in our image. Fuzzy classification means we have to classify into fuzzy continuous value. [[Pasted image 20230321171655.png]]\n\n\n7.3.2 How’s the pixel content?\nsecond, we have to be careful whether we are going to see sub pixel value, whole general pixel value, or some pixel value. It is important te be a concern because it’s quiet rare to have a completely homogenous pixel. Sometimes we have to find the majority value of the pixel. [[Pasted image 20230321171932.png]] source: citation # The pixel: A snare and a delusion\n\n\n7.3.3 Pixel or Object?\nLast, we have to consider are we going to look at pixel or object on the image data. Example of pixel are shon in image on the left, while example of object shown on the right image. It is important because our data observation will be different between pixel or object. [[Pasted image 20230321195605.png]] source: Superpixels supercells\nIt is important to define this constraint before we do classificatio because we are the one who will select the propriate methods and ask the machine.\nAfter considering those constrain we should select the propriate methods for our classification. Working with machine, make us temped to work with complicated model. However, not every condition are required complicated methods. Complicated methods might have higher accuracy but it will be harder to interprate. I present this illustration to show the trade off between accuracy and interpretability.\n[[Pasted image 20230321200314.png]] source: citation Support Vector Machine vs. Random Forest for Remote Sensing Image Classification: A Meta-analysis and systematic review\nPlease choose classification methods wisely."
  },
  {
    "objectID": "week06.html#natural-disaster-vulnerability-map-using-support-vector-machine-svm",
    "href": "week06.html#natural-disaster-vulnerability-map-using-support-vector-machine-svm",
    "title": "6  week_06",
    "section": "8.1 Natural Disaster Vulnerability map using Support Vector Machine (SVM)",
    "text": "8.1 Natural Disaster Vulnerability map using Support Vector Machine (SVM)\n\n8.1.1 Flood\nNatural disaster can cause devastating impact on economy, social, and environment. Flood is one of the most destructive disaster that happen frequently in some areas in the world. It force local community and government to manage the disaster and be resilience towards it. Therefore, recognizing flood-prone areas is essential initial step in planning flood management and resilience strategy. Flood is a dynamic and complex disaster that is caused by a lot of factors. To assessing flood vlunerable area, we have to put many things into account such as, rainfall data, land cover, topography, and so on. One of supervised learning’s benefit is effective in high dimension (SupportVectorMachines?). (youssefFloodVulnerabilityMapping2023a?) used this advantage to assess flood phenomena in Taif in Saudi Arabia. The study showed combination 13 parameters, including remote sensing data, and Support Vector Machine (SVM), can make a vulnerability maps like shown in the image below. [[Pasted image 20230324010018.png]] This vulnerability map can be used by policy maker to pointed action plan to prevent and mitigate flooding. Different class in this vulnerabiliity map guides planner to understand which area are highly effected by flood.\nOther than SVM, this study has tried to use bivariate and multivariate methods. Apparently, SVM model performed best with 96.2% accuracy. Earllier, (mojaddadiEnsembleMachinelearningbasedGeospatial2017?) made flood vulnerability map using SVM as well for Damansara, Malaysia. The result also indicates high classification accuracy with 84.07%. Although in the summary SVM were shown has low interpretability, this study show that using the right data and combination of methods we can get high accuracy and easy to understand result.\n\n\n8.1.2 Drought\nnot only for flood, supervised classification also had been used to assess other natural disaster. (ghasempourDroughtVulnerabilityAssessment2022?) made Drought vulnerability maps northwest part of Iran. The study used SVM on 17 geo-environmental parameters including temperature and soil condition which are not included by (mojaddadiEnsembleMachinelearningbasedGeospatial2017?) and (youssefFloodVulnerabilityMapping2023a?) in their researches.\n[[Pasted image 20230324014809.png]] Drought Vulnerability Map of Northwest Iran source: (ghasempourDroughtVulnerabilityAssessment2022?)"
  },
  {
    "objectID": "week07.html",
    "href": "week07.html",
    "title": "7  week_07",
    "section": "",
    "text": "8 Summary\nIn the previous week we have discussed about image classification that operated per pixel. Howeever there are some drawbacks of this method which we are neglacting adjacency aspect meanwhile in geographic law, anything is related and near things are more related. Moreover, it is rare to have pure homogeneous pixel which mean in a single pixel it can consist of various land cover. Recalling a little bit from last week learning diary, at the end of summary I have mention some consideration before we classify EO image. One of the consideration is whether we are going to classify object or pixel or even sub pixel. In this week we still going to discuss about image classification methods, we are going to discuss about other image classifications which are not operate on pixel base. Instead of discusing classification in pixel based, we are going to discuss Object based Image Analysis (OBIA) and sub-pixel Analysis.\nLearning about further image classification based on object and sub-pixel are really interesting because we can develop our analysis further. I will discuss examples of OBIA and sub-pixel analysis."
  },
  {
    "objectID": "week07.html#object-based-image-analysis-obia",
    "href": "week07.html#object-based-image-analysis-obia",
    "title": "7  week_07",
    "section": "8.1 Object Based Image Analysis (OBIA)",
    "text": "8.1 Object Based Image Analysis (OBIA)\nAs we discuss above, per pixel classification just take individual pixel into account, it is not considering adjacency and similarity. Object based analysis comes as an alternative that instead of operating classification per pixel, it is better to make it as per object. Objects or Superpixels or Supercells is shape on the image that grouped together based on the similarity (homogeneity) of adjacent pixels. Ultimately, the idea of OBIA is turn image into smaller objects, calculate mean value of each objects and classify the object.\nHowever. How to turn our image into objects or superpixels? one of the most common method is SLIC (Simple Linear Iterative Clustering). Here is steps of superpixels generation using SLIC:\n\nGiven an image\nPlace number of points in the image\ncheck on parameter which are distance and similarity. Those points will become centroid to:\n\nLook at the distance of two pixels\nLook at the similarity (homogeneity) of near pixels\n\nIf these 2 parameters are accomplished, then a boundaries will be drawn to identify as one object\nIteration by changing the distance and similarity parameters\n\nThe hyperparameter or what we can set for this object generation iteration are: - distance between 2 points (s) - compactness (m) or balance between pixel distance and colour\nThe output is objectified image like shown below. [[Pasted image 20230321230720.png]] source: SegOptim performing object-based image classification\nThis is not the final output of our classification but it will be the input for classification method. After generating this image we can calculate the average value of each object and use those value to do classification as we learn last week."
  },
  {
    "objectID": "week07.html#sub-pixel-analysis",
    "href": "week07.html#sub-pixel-analysis",
    "title": "7  week_07",
    "section": "8.2 Sub-pixel Analysis",
    "text": "8.2 Sub-pixel Analysis\nIn EO imagery, it is difficult to have homogeneous pixel. A pixel can contain many different information or land cover type. Thus, it will be beneficial to learn about Sub Pixel Analysis because we can see percentage of different land cover type on a pixel.\nSub pixel analysis is also known as sub pixel classification, spectral mixture analysis (SMA), or linear spectral unmixing. We treat pixels as combination of land cover fractions. In this analysis, ultimately, we take an individual pixel and estimate fraction (percentage) that make up that pixels. for example we will estimate how many percent vegetation, water, and bare soil in the pixel we have. the image below is the example that a pixel is a mixing of 70% vegetation, 20% water, and 10% bare soil [[Pasted image 20230321234011.png]]\nThe way we calculate percentage of fraction is by taking Spectrally Pure Endmember. Spectrally Pure Endmember is pixel that has homogeneous land cover or the purest pixel that has nothing else within it. However, it is difficult to have pure endmember. Thus, sometimes we use spectral library as the pure endmember. The pixel we have is a combination of various endmember.\nAlthough machine will do the calculation for us, it will be better to understand whats going on to calculate sub pixel analysis. What we are going to find is percentage of each endmember. So here is the step (to make the ): 1. the basic formula is our band value equals to endmember’s band value multiply with fraction proportion. - [[Pasted image 20230322004431.png]] - pλ is our pixel band value - piλ is band value of endmember - fi is percentage of fraction 2. To make the explanation easier we will use band 3 and 4 from our pixel and endmember. Since we use 2 bands we then turn the formula into a matrix which will look like this - [[Pasted image 20230322004457.png]] 3. Since we want to know the f value for each fraction of endmember, we have to invers the matrix. Thus, the equation will look like this. (ps. we already put the band value) 4. the calculation will give us the percentage of each end member\nThe output is percentage of each endmember. Using the given percentage we can make a map that has fractions. It will be usefull because we can analyse fraction of each endmember on particular area. We can use the result for analysis with other aspect, such as air pollution, public health, etc.\nThere are various ways to identify combination of endmember fraction. One of the common ways is using V-I-S model. VIS model means Vegetation-Impervious surface-Soil (V-I-S) fractions. it is shown in the triangle diagram below. We can identify pixel’s land cover based on this VIS triangle. [[Pasted image 20230322005535.png]]"
  },
  {
    "objectID": "week07.html#sub-pixel-analysis-application",
    "href": "week07.html#sub-pixel-analysis-application",
    "title": "7  week_07",
    "section": "9.1 Sub-pixel Analysis Application",
    "text": "9.1 Sub-pixel Analysis Application\n\n9.1.1 Multiple endmember spectral mixture analysis (MESMA)\nReal advantage of sub-pixel analysis is we can see the fraction of a pixel. We can utilise this benefit by classifying spectrum of a pixel so instead of having binary classification we can have more continuous value. (2009), utilise the benefit of sub-pixel analysis to develop a new method on estimating size and tempertarute of forest fire. Previously, Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) were utilised to detect binary whether there is fire or no fire. Proposed methods used Multiple endmember spectral mixture analysis (MESMA) to estimate subpixels, fires sizes, and temperatures of forest fire that happend in California on 14 September 2006.\nThe methods applied MESMA on ASTER night-time image on 15 September 2006. Simillar to Spectral Mixture Analysis (SMA), MESMA analyse sub-pixel or fraction of pixel, but this methods can do analysis using more then 2 bands. Thus, MESMA can utilise the advantage of ASTER which is has more than 2 band by involving band 4, 5, 6, 7, 8, and 9.\n[[Pasted image 20230323201143.png]] source:\nThe output of this analysis is maps of fire size (image a) and temperature (image b). However, this research assumes only one single fire temperature per pixel. Estimating fire temperature will lead to another research because fire temperature influence gas and aerosol emission, ecological impact, and spreading rates. MESMA methods can be beneficial for countries that has a lot of peatland like Indonesia and Malaysia, because it is prone to combustion. Country governement can use this methods on near-real time dataset to handle fire combustion.\n\n\n9.1.2 Urban Surface Emissivity\nSimilar methods on using sub-pixel analysis to estimate temperature also done by (2012) in mapping urban surface emissivity in Crete, Grece. However, this research only needed to use ASTER image band 13. Thus, instead of using MESMA, it only use SMA. Surface emissivity (or temperature) is really depend on the surface type and physical condition so it is good use sub-pixel analysis. The benefit of using this method is ability to hadle mixed pixel so we can capture various biophysical parameter in a pixel.\n[[Pasted image 20230323215521.png]] source: (2012)\nLearning about sub-pixel analsis on surface emissivity is really beneficial because high earth temperature is a global issue that being faced by every country. Surface temperature increase can caused the increase of energy consumption up to 8.5% and affecting citizen health and well being (Santamouris et al., 2015). Making surface emissivity map helps city planner and decision maker finding the right solution to cool down the earth. Although, this study present a really beneficial methods, it used only one date image. Meanwhile in assessing heat in a city we might need maximum temperature value. Further study has to be done to check whether this methodology robust or not. Therefore, in the further study of this methods we have to use dataset various date to have a better result."
  },
  {
    "objectID": "week08.html",
    "href": "week08.html",
    "title": "8  week_08",
    "section": "",
    "text": "9 Application\nTo help decision makers and cityplanners mitigating UHI phenomena, a reliable UHI vulnerability map is required. UHI vulnerability maps tells us which area in the city that are at risk of UHI. Sidiqui, (2022) used high-resolution spatial data, advanced geospatial tools, and socio-demographic data to make UHI vulnerabilty map ofthe City of Greater Geelong, Australia. It utilise airborne thermal data from Landsat and aerial imagery data to make UHI index maps which then indicated significant hotspot on high building density, inductrial areas, and constructed sites. The UHI index maps, then combined with 20 input indicators of heat exposure, population sensitity, and mobility/adaptive capacity to make Urban Heat Vulnerability (UHV) maps.\n[[Pasted image 20230323032502.png]]\nsource: (Sidiqui et al., 2022)\nHigh vulnerability maps are shown with red (warmer) colour which concentrated on high building density and high population density areas.\nThis proposed methodology show comprehensive approach by considering multitemporal input data. The landsat scene data were taken from hotest days of summer in 2016 and 2017. (Técher, Ait Haddou and Aguejdad, 2023) has done the similar methodolgy to assess UHI in # Montpellier Méditerranée, France but with 15 indicators. It indicates that this methodology is reproducible.\nMitigating surface urban heat island by a tree protection policy: A case study of The Woodland, Texas, USA - ScienceDirect"
  },
  {
    "objectID": "week08.html#global-policy",
    "href": "week08.html#global-policy",
    "title": "8  week_08",
    "section": "8.2 Global Policy",
    "text": "8.2 Global Policy\nUrban Heat Island happens globally. A lot of global policy has implicitly and explicitly put this issue into an account. One of the target in goal number 11 of Sustainable Development Goals mention:\n\nCleaner, greener cities - Investing in parks and green spaces in urban areas will help to amelioratethe urban heat island effect and improve air quality in urban spaces.\n\n\n8.2.1 Beat the heat handbook\nMoreover, United Nation Environment Program has released Beat the Heat handbook. The hand book gives guideline and framework for planner to help cooling their cities. it gives systemic approach to address urban cooling.\n[[Pasted image 20230323010332.png]]\nsource: Beating the Heat: A Sustainable Cooling Handbook for Cities | NDC Action Project (unep.org)\nIt also has matrix of city interventions for sustainable urban cooling like shown in the image below\n[[Pasted image 20230323010142.png]] Source: [Beating the Heat: A Sustainable Cooling Handbook for Cities (https://www.unep.org/resources/report/beating-heat-sustainable-cooling-handbook-cities)\nAlthough this handbook presents various intervention alternative with over 20 cities as case study and examples, it is not easy for local government to follow because different cities has different heat temperature and different context. As mentioned earlier that geography can cause different heat in urban context. Dissimilar context and issue should have different policy and planning to solve Urban Heat Island issue. Therefore, assessment on existing environmetal condition has to be done with right method and right approach to uderstand specific solution that the city needs."
  },
  {
    "objectID": "week08.html#utilising-eo-data",
    "href": "week08.html#utilising-eo-data",
    "title": "8  week_08",
    "section": "8.3 Utilising EO data",
    "text": "8.3 Utilising EO data\nto assess heat in the city we can utilise earth observation data, like what Fremantle did in their Urban Forest Plan. However, if we are not critical in using EO data for assessing heat, it will fail telling the pain point comprehansively like Fremantle. In fremantle’s assessment process, they only use one temperature image and aggregated the data. Moreover, it is not reproducable and not clear on what kind of assumption they made in their assessment process.\nThus, in the application part we will discuss some previous research that have utilised EO data and linked with policies to tackle UHI"
  },
  {
    "objectID": "week08.html#summary",
    "href": "week08.html#summary",
    "title": "8  week_08 - Temperature and Policy",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nIn this week we will discuss about Urban heat Island and how city handle this issue. A phenomena when urban area is significantly hotter than rural area on its surrounding is called Urban Heat Island (UHI) (Takebayashi and Moriyama, 2020).\n source: Earth.Org\nUrban heat island are caused by many factors: - Limited vegetation or natural landscape - Urban material absorb and emit solar heat - Building dimensions and configuration trap wind flow that absorb and release heat - Human activities, such as, vehicle and AC usage - Weather and geography Generally, highly concentrated man-made material and limited natural landscape is the major factors of UHI (US EPA, 2014).\nUrban Heat Island bring a lot of negative impact on social, economy and environmental globally. For example, heatwave in 1998 in Shanghai increase the mortality rate in urban area up to 27.3/100.000, compared to 7/100.000 in rural area []. Heatwaves in 2003, killed 35.000 lives in Europe with 15.000 fatalities in France alone (Tan et al., 2010). Santamouris (2015), stated that one degree increase of temperature, potentially increase of electrical demand between 0.5%-8.5%.\n\n8.1.1 Global Policy\nUrban Heat Island happens globally. A lot of global policy has implicitly and explicitly put this issue into an account. One of the target in goal number 11 of Sustainable Development Goals mention:\n\nCleaner, greener cities\n- Investing in parks and green spaces in urban areas will help to amelioratethe urban heat island effect and improve air quality in urban spaces.\n\n\n8.1.1.1 Beat the heat handbook\nMoreover, United Nation Environment Program has released Beat the Heat handbook. The hand book gives guideline and framework for planner to help cooling their cities. it gives systemic approach to address urban cooling.\n\n\n\nBeat the Heat Roadmap\n\n\nsource: Beating the Heat: A Sustainable Cooling Handbook for Cities | NDC Action Project (unep.org)\nIt also has matrix of city interventions for sustainable urban cooling like shown in the image below\n Source: [Beating the Heat: A Sustainable Cooling Handbook for Cities (https://www.unep.org/resources/report/beating-heat-sustainable-cooling-handbook-cities)\nAlthough this handbook presents various intervention alternative with over 20 cities as case study and examples, it is not easy for local government to follow because different cities has different heat temperature and different context. As mentioned earlier that geography can cause different heat in urban context. Dissimilar context and issue should have different policy and planning to solve Urban Heat Island issue. Therefore, assessment on existing environmental condition has to be done with right method and right approach to understand specific solution that the city needs.\n\n\n\n8.1.2 Utilising EO data\nto assess heat in the city we can utilise earth observation data, like what Fremantle did in their Urban Forest Plan. However, if we are not critical in using EO data for assessing heat, it will fail telling the pain point comprehensively like Fremantle. In Fremantle’s assessment process, they only use one temperature image and aggregated the data. Moreover, it is not reproducible and not clear on what kind of assumption they made in their assessment process.\nThus, in the application part we will discuss some previous research that have utilised EO data and linked with policies to tackle UHI"
  },
  {
    "objectID": "week08.html#application",
    "href": "week08.html#application",
    "title": "8  week_08 - Temperature and Policy",
    "section": "8.2 Application",
    "text": "8.2 Application\n\n8.2.1 Urban Heat Vulnerability\nTo help decision makers and city planners mitigating UHI phenomena, a reliable UHI vulnerability map is required. UHI vulnerability maps tells us which area in the city that are at risk of UHI. Sidiqui et al. (2022) used high-resolution spatial data, advanced geospatial tools, and socio-demographic data to make UHI vulnerability map ofthe City of Greater Geelong, Australia. It utilise airborne thermal data from Landsat and aerial imagery data to make UHI index maps which then indicated significant hotspot on high building density, industrial areas, and constructed sites. The UHI index maps, then combined with 20 input indicators of heat exposure, population sensitivity, and mobility/adaptive capacity to make Urban Heat Vulnerability (UHV) maps.\n\n\n\nUrban Heat Vulnerability Map\n\n\nsource: (Sidiqui et al., 2022)\nHigh vulnerability maps are shown with red (warmer) colour which concentrated on high building density and high population density areas.\nThis proposed methodology show comprehensive approach by considering multitemporal input data. The landsat scene data were taken from hotest days of summer in 2016 and 2017. (Técher, Ait Haddou and Aguejdad, 2023) has done the similar methodology to assess UHI in # Montpellier Méditerranée, France but with 15 indicators. It indicates that this methodology is reproducible.\nMitigating surface urban heat island by a tree protection policy: A case study of The Woodland, Texas, USA - ScienceDirect"
  },
  {
    "objectID": "week08.html#reflection",
    "href": "week08.html#reflection",
    "title": "8  week_08 - Temperature and Policy",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\n\nfrom this week we have learnt that Urban heat Island is a global phenomena that has to be mitigated immediately. Some of global policy has take this issue into an account. United Environment program has release Beat the Heat Handbook as a guideline for planner to help cooling their cities. Although the handbook suggested a lot of intervention and example from cities around the world. Local government might having difficulties to follow the guideline because it has a lot of pages and different urban context might require different intervention.\nThus, EO data can be utilised to assess UHI vulnerability in a city like it was done by (Sidiqui et al., 2022)\nSidiqui et al. (2022) application are really helpful to assess the city temperature context and it shown that it is reproducible. What local government can do is adjusting 20 indicators of this methods to adapt with their city context.\nMoreover, data that being used in making UH vulnerability maps is free so developing countries who usually has limited budget can replicate this method\nhowever, making vulnerability maps might not solve the problem right away but it can bring a really beneficial information. Thus, local government can make a specific action plan based on vulnerability and urgency degree.\n\n\n\n\n\nSantamouris, M. et al. (2015) “On the impact of urban heat island and global warming on the power demand and electricity consumption of buildings—A review,” Energy and Buildings, 98, pp. 119–124. doi: 10.1016/j.enbuild.2014.09.052.\n\n\nSidiqui, P. et al. (2022) “Urban Heat Island vulnerability mapping using advanced GIS data and tools,” Journal of Earth System Science, 131(4), p. 266. doi: 10.1007/s12040-022-02005-w.\n\n\nTakebayashi, H. and Moriyama, M. (2020) “Chapter 1 - Background and purpose,” in Takebayashi, H. and Moriyama, M. (eds.) Adaptation Measures for Urban Heat Islands. Academic Press, pp. 1–8. doi: 10.1016/B978-0-12-817624-5.00001-4.\n\n\nTan, J. et al. (2010) “The urban heat island and its impact on heat waves and human health in Shanghai,” International Journal of Biometeorology, 54(1), pp. 75–84. doi: 10.1007/s00484-009-0256-x.\n\n\nTécher, M., Ait Haddou, H. and Aguejdad, R. (2023) “Urban Heat Island’s Vulnerability Assessment by Integrating Urban Planning Policies: A Case Study of Montpellier Méditerranée Metropolitan Area, France,” Sustainability, 15(3, 3), p. 1820. doi: 10.3390/su15031820.\n\n\nUS EPA, O. (2014) Learn About Heat Islands. Available at: https://www.epa.gov/heatislands/learn-about-heat-islands (Accessed: March 23, 2023)."
  },
  {
    "objectID": "week07.html#summary",
    "href": "week07.html#summary",
    "title": "7  week_07 - Classification Continue",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nIn the previous week we have discussed about image classification that operated per pixel. However there are some drawbacks of this method which we are neglacting adjacency aspect meanwhile in geographic law, anything is related and near things are more related. Moreover, it is rare to have pure homogeneous pixel which mean in a single pixel it can consist of various land cover. Recalling a little bit from last week learning diary, at the end of summary I have mention some consideration before we classify EO image. One of the consideration is whether we are going to classify object or pixel or even sub pixel. In this week we still going to discuss about image classification methods, we are going to discuss about other image classifications which are not operate on pixel base. Instead of discussing classification in pixel based, we are going to discuss Object based Image Analysis (OBIA) and sub-pixel Analysis.\n\n7.1.1 Object Based Image Analysis (OBIA)\nAs we discuss above, per pixel classification just take individual pixel into account, it is not considering adjacency and similarity. Object based analysis comes as an alternative that instead of operating classification per pixel, it is better to make it as per object. Objects or Superpixels or Supercells is shape on the image that grouped together based on the similarity (homogeneity) of adjacent pixels. Ultimately, the idea of OBIA is turn image into smaller objects, calculate mean value of each objects and classify the object.\nHowever. How to turn our image into objects or superpixels? one of the most common method is SLIC (Simple Linear Iterative Clustering). Here is steps of superpixels generation using SLIC:\n\nGiven an image\nPlace number of points in the image\ncheck on parameter which are distance and similarity. Those points will become centroid to:\n\nLook at the distance of two pixels\nLook at the similarity (homogeneity) of near pixels\n\nIf these 2 parameters are accomplished, then a boundaries will be drawn to identify as one object\nIteration by changing the distance and similarity parameters\n\nThe hyperparameter or what we can set for this object generation iteration are: - distance between 2 points (s) - compactness (m) or balance between pixel distance and colour\nThe output is objectified image like shown below.\n\n\n\nObject BAsed Image Analysis\n\n\nsource: SegOptim performing object-based image classification\nThis is not the final output of our classification but it will be the input for classification method. After generating this image we can calculate the average value of each object and use those value to do classification as we learn last week.\n\n\n7.1.2 Sub-pixel Analysis\nIn EO imagery, it is difficult to have homogeneous pixel. A pixel can contain many different information or land cover type. Thus, it will be beneficial to learn about Sub Pixel Analysis because we can see percentage of different land cover type on a pixel.\nSub pixel analysis is also known as sub pixel classification, spectral mixture analysis (SMA), or linear spectral unmixing. We treat pixels as combination of land cover fractions. In this analysis, ultimately, we take an individual pixel and estimate fraction (percentage) that make up that pixels. for example we will estimate how many percent vegetation, water, and bare soil in the pixel we have. the image below is the example that a pixel is a mixing of 70% vegetation, 20% water, and 10% bare soil\n\n\n\nFraction concept\n\n\nsource: Pérez Machado and Small (2013)\nThe way we calculate percentage of fraction is by taking Spectrally Pure Endmember. Spectrally Pure Endmember is pixel that has homogeneous land cover or the purest pixel that has nothing else within it. However, it is difficult to have pure endmember. Thus, sometimes we use spectral library as the pure endmember. The pixel we have is a combination of various endmember.\nAlthough machine will do the calculation for us, it will be better to understand whats going on to calculate sub pixel analysis. What we are going to find is percentage of each endmember. So here is the step to calculate fraction: 1. the basic formula is our band value equals to endmember’s band value multiply with fraction proportion.\n\n\n\nFraction formula\n\n\n- *pλ* is our pixel band value\n- *piλ* is band value of endmember\n- *fi* is percentage of fraction\n\nTo make the explanation easier we will use band 3 and 4 from our pixel and endmember. Since we use 2 bands we then turn the formula into a matrix which will look like this\n\n\n\n\nFraction matrix\n\n\n\nSince we want to know the f value for each fraction of endmember, we have to invers the matrix. Thus, the equation will look like this. (ps. we already put the band value)\nthe calculation will give us the percentage of each end member\n\nThe output is percentage of each endmember. Using the given percentage we can make a map that has fractions. It will be usefull because we can analyse fraction of each endmember on particular area. We can use the result for analysis with other aspect, such as air pollution, public health, etc.\nThere are various ways to identify combination of endmember fraction. One of the common ways is using V-I-S model. VIS model means Vegetation-Impervious surface-Soil (V-I-S) fractions. it is shown in the triangle diagram below. We can identify pixel’s land cover based on this VIS triangle.\n\n\n\nV-I-S Model\n\n\nsource: Plaza et al. (2002)"
  },
  {
    "objectID": "week07.html#application",
    "href": "week07.html#application",
    "title": "7  week_07 - Classification Continue",
    "section": "7.2 Application",
    "text": "7.2 Application\nLearning about further image classification based on object and sub-pixel are really interesting because we can develop our analysis further. I will discuss examples of OBIA and sub-pixel analysis.\n\n7.2.1 Sub-pixel Analysis Application\n\n7.2.1.1 Multiple endmember spectral mixture analysis (MESMA)\nReal advantage of sub-pixel analysis is we can see the fraction of a pixel. We can utilise this benefit by classifying spectrum of a pixel so instead of having binary classification we can have more continuous value. Eckmann (2009), utilise the benefit of sub-pixel analysis to develop a new method on estimating size and temperature of forest fire. Previously, Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) were utilised to detect binary whether there is fire or no fire. Proposed methods used Multiple endmember spectral mixture analysis (MESMA) to estimate subpixels, fires sizes, and temperatures of forest fire that happen in California on 14 September 2006.\nThe methods applied MESMA on ASTER night-time image on 15 September 2006. Similar to Spectral Mixture Analysis (SMA), MESMA analyse sub-pixel or fraction of pixel, but this methods can do analysis using more then 2 bands. Thus, MESMA can utilise the advantage of ASTER which is has more than 2 band by involving band 4, 5, 6, 7, 8, and 9.\n\n\n\n(a) Fire temperature, (b) Fire size\n\n\nsource: Eckmann, Roberts and Still (2009)\nThe output of this analysis is maps of fire size (image a) and temperature (image b). However, this research assumes only one single fire temperature per pixel. Estimating fire temperature will lead to another research because fire temperature influence gas and aerosol emission, ecological impact, and spreading rates. MESMA methods can be beneficial for countries that has a lot of peatland like Indonesia and Malaysia, because it is prone to combustion. Country governement can use this methods on near-real time dataset to handle fire combustion.\n\n\n7.2.1.2 Urban Surface Emissivity\nSimilar methods on using sub-pixel analysis to estimate temperature also done by Mitraka (2012), in mapping urban surface emissivity in Crete, Grece. However, this research only needed to use ASTER image band 13. Thus, instead of using MESMA, it only use SMA. Surface emissivity (or temperature) is really depend on the surface type and physical condition so it is good use sub-pixel analysis. The benefit of using this method is ability to hadle mixed pixel so we can capture various biophysical parameter in a pixel.\n\n\n\nSub pixel Map\n\n\nsource: Mitraka et al. (2012)\nLearning about sub-pixel analysis on surface emissivity is really beneficial because high earth temperature is a global issue that being faced by every country. Surface temperature increase can caused the increase of energy consumption up to 8.5% and affecting citizen health and well being (Santamouris et al., 2015). Making surface emissivity map helps city planner and decision maker finding the right solution to cool down the earth. Although, this study present a really beneficial methods, it used only one date image. Meanwhile in assessing heat in a city we might need maximum temperature value. Further study has to be done to check whether this methodology robust or not. Therefore, in the further study of this methods we have to use dataset various date to have a better result."
  },
  {
    "objectID": "week07.html#reflection",
    "href": "week07.html#reflection",
    "title": "7  week_07 - Classification Continue",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\n\nLearning about OBIA and sub pixel analysis are really beneficial because there are some cases that per pixel image classification is the best fit.\nObject based Image Analysis we are considering pixel simmilarity and adjacency, so it will be better to have object classification\nLearning about sub pixel analysis is really beneficial because in a single pixel mostly it always contain various land cover. Using sub pixel analysis we can specify mixing of various land cover in a pixel. It gives us continuos value, instead of binary category like shown in the fire detection.\nUnderstanding fire temperature estimation using MESMA are really beneficial for tropical country that has a big forest or peatland are. Local government can utilize this to mitigate forect combustion.\nOverheatting earth surface is global issue that affecting social, economy and environmental aspects. Thus, every countries government try to cool down their land. However, to take a right decision, local government has to assess the temperature based on their surface type and physical condition. SMA methods is really beneficial here because it can helps local governement estimating their surface temperature\nWe have to be critical on the research or paper we see because an innovative methods is not always a robust replicable one. We have to check did this methods have been studied in the right manner and do this methods appropriate for our problem.\n\n\n\n\n\nEckmann, T. C., Roberts, D. A. and Still, C. J. (2009) “Estimating subpixel fire sizes and temperatures from ASTER using multiple endmember spectral mixture analysis,” International Journal of Remote Sensing, 30(22), pp. 5851–5864. doi: 10.1080/01431160902748531.\n\n\nMitraka, Z. et al. (2012) “Improving the estimation of urban surface emissivity based on sub-pixel classification of high resolution satellite imagery,” Remote Sensing of Environment, 117, pp. 125–134. doi: 10.1016/j.rse.2011.06.025.\n\n\nPérez Machado, R. and Small, C. (2013) “IDENTIFYING MULTI-DECADAL CHANGES OF THE SAO PAULO URBAN AGGLOMERATION WITH MIXED REMOTE SENSING TECHNIQUES: SPECTRAL MIXTURE ANALYSIS AND NIGHT LIGHTS,” EARSeL eProceedings, 12, pp. 101–112. doi: 10.12760/01-2013-2-03.\n\n\nPlaza, A. et al. (2002) “Spatial/spectral endmember extraction by multidimensional morphological operations,” IEEE Transactions on Geoscience and Remote Sensing, 40(9), pp. 2025–2041. doi: 10.1109/TGRS.2002.802494.\n\n\nSantamouris, M. et al. (2015) “On the impact of urban heat island and global warming on the power demand and electricity consumption of buildings—A review,” Energy and Buildings, 98, pp. 119–124. doi: 10.1016/j.enbuild.2014.09.052."
  },
  {
    "objectID": "week06.html#summary",
    "href": "week06.html#summary",
    "title": "6  week_06 - Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nIn this week we learn about Classification, this week topic is related to module CASA0006 Data Science for Spatial System. As we know remote sensing collect imagery satellite data, but what we can do to identify information in the imagery data? how to identify what happening in the image? What is land use and land cover of the given image? is it forest are or urban are?\nWe can identify the information using Image classification methods. The image classifiation is done computationally. If we see an imagery data we can identify or classify the object within the image by judging based on our experience or called Inductive Learning. Using that idea, now we train the computer system to have human knowledge to solve problem called Machine Learning. We can train computer system to learn on how to identifying satellite imagery data using classification methods.\n\nIn this week, actually we also learn about Classification and Regression Tree (CART). However, it has been covered in module CASA0006 - Data Science for Spatial System. Thus, instead of discussing about it, I will discuss something new that I learnt which are Supervised and Unsupervised Learning\n\nin Statistic, classification ideas is basically assigning our data into certain categories. Image Classification is basically turn every pixel on the image into one categorical classification. for example our pixel can be identified as forest or residential, high or low level forest combustion and so on. There are 2 types of image classification which are Supervised and unsupervised. Image classification has been being used since 1970. It was started with unsupervised in 1970s, to supervised classification in 1975s and now mostly we have been using Object-Based Image Analysis since 1995s. But what is supervised and unsupervised classification? Check this table for general idea of those 2 classification. But I will tell further in the explanation later.\n\n\n\n\n\n\n\nUnsupervised\nSupervised\n\n\n\n\nWe give certain data. After that we ask for certain mount of classes and the computer (machine) will return with that amount of classes we asked\nwe give training dataset and the model. After that we ask to classify the rest of the data or image\n\n\nNo labelled data\nSome training data has been labelled\n\n\n\nNow, we will discuss one by one.\n\n6.1.1 Unsupervised Learning\nUnsupervised Learning is using machine to analyse or cluster our data. So what usually will happen is we give our data, then we ask it to be cluster the dat into certain number of cluster. The machine will return with cluster of data ask we requested. In the context of imagery satellite data, the given data is value of band for each pixel.\nUnsupervised learning usually referred to clustering/k-means. In clustering, we have spectral feature space by plotting 2 or more bands value against each other. If we only have 2 bands values, the pixel value will be plotted in 2 dimension feature space. As we add more band value, the dimension will be added as well. After we give our data, we have to set some rules to tell the machine how the data should be clustered. The machine then will start random center point of cluster with radius that has been set. Pixels value within same radius are in the same cluster. machine will start moving it’s center point until all pixels has been allocated to a cluster.\nDetail steps of clustering: 1. Give data 2. Then set series of rules, such as: - the radius in spectral feature to define where new cluster started - spectral distance measure - number of pixels to be considered before merging - max number of cluster 3. It will repeat until no pixel allocated again\n\n\n\nK-mean Clustering\n\n\nsource: Wan(2017)\nOther methods of unsupervised learning is ISODATA. This methods is specifically for earth observation data. This methods is similar to K-means clustering but with some additional rules, which are: - Any clusters have so few pixels = meaningless - Clusters are so close they can be merged - Clusters can be split - elongated clusters in feature space\nAlthough learning using ISODATA is beneficial because it’s specifically used for EO data, this methods has a drawback which is can create a lots of clusters and difficult to assign meaning. For example it can cause two types of land cover in a pixel. To tackle this drawbacks we can do Cluster busting which is basically mask cluster that incorrect and label to new one.\n\n\n\nISODATA\n\n\n\n\n6.1.2 Supervised Learning\nDifferent from unsupervised learning, supervised learning uses labelled training data. Training data is dataset that being used to train the model, so the model can classify the data for us. Other than training data, we have testing data. Testing data is dataset that being used to assess our model. Training and testing data has to be mutually exclusive.\nGenerally steps of supervised learning are: 1. Class Definition 2. Pre-processing 3. Training 4. Pixel Assignment 5. Accuracy assignment\nMethods on supervised learning are categorized into 2:\n\nParametric, use data that are normally distributed\n\nMethods in parametric:\n\nMaximum likelihood\n\n\nNon-parametric, uses data that are not normally distributed\n\nMethods in parametric:\n\nDensity slicing\nParallelpiped\nMinimum distance to mean\nNearest neighbor\nNeural networks\nMachine learning / expert systems (eg. Support Vector Machine, Neural Networks)\n\n\n\nIt is quiet rare to have normally distributed data in earth observation image. Moreover, recently most works uses machine learning/expert systems or spectral mixture analysis.\n\n\n6.1.3 Be Aware\nEven though, image classification will be done by machine, as researcher or the one who assign the machine we have to be aware of several things:\n\n6.1.3.1 Hard Classification or Fuzzy Classification?\nFirst we have to be aware, are we going to do hard classification or fuzzy classification. Hard classification means there will be a define category like water or land in our image. Fuzzy classification means we have to classify into fuzzy continuous value.\n\n\n\nHard and Fuzzy Classification\n\n\nsource: Jensen, page 413\n\n\n6.1.3.2 How’s the pixel content?\nsecond, we have to be careful whether we are going to see sub pixel value, whole general pixel value, or some pixel value. It is important te be a concern because it’s quiet rare to have a completely homogeneous pixel. Sometimes we have to find the majority value of the pixel.\n\n\n\npixel content\n\n\nsource: Fisher (1997)\n\n\n6.1.3.3 Pixel or Object?\nLast, we have to consider are we going to look at pixel or object on the image data. Example of pixel are shown in image on the left, while example of object shown on the right image. It is important because our data observation will be different between pixel or object.\n\n\n\n(right) Pixel (left) Object\n\n\nsource: Superpixels supercells\nIt is important to define this constraint before we do classification because we are the one who will select the appropriate methods and ask the machine.\nAfter considering those constrain we should select the appropriate methods for our classification. Working with machine, make us temped to work with complicated model. However, not every condition are required complicated methods. Complicated methods might have higher accuracy but it will be harder to interprate. I present this illustration to show the trade off between accuracy and interpretability.\n\n\n\nTrade off accuracy and interpretability\n\n\nsource: Sheykhmousa and Mahdianpari (2020)\n\nPlease choose classification methods wisely!"
  },
  {
    "objectID": "week06.html#application",
    "href": "week06.html#application",
    "title": "6  week_06 - Classification",
    "section": "6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Natural Disaster Vulnerability map using Support Vector Machine (SVM)\n\n6.2.1.1 Flood\nNatural disaster can cause devastating impact on economy, social, and environment. Flood is one of the most destructive disaster that happen frequently in some areas in the world. It force local community and government to manage the disaster and be resilience towards it. Therefore, recognizing flood-prone areas is essential initial step in planning flood management and resilience strategy. Flood is a dynamic and complex disaster that is caused by a lot of factors. To assessing flood vulnerable area, we have to put many things into account such as, rainfall data, land cover, topography, and so on. One of supervised learning’s benefit is effective in high dimension (1.4. Support Vector Machines, no date). Youssef et al. (2023) used this advantage to assess flood phenomena in Taif in Saudi Arabia. The study showed combination 13 parameters, including remote sensing data, and Support Vector Machine (SVM), can make a vulnerability maps like shown in the image below.\n\n\n\nFlood vulnerable map\n\n\nsource: Youssef et al. (2023)\nThis vulnerability map can be used by policy maker to pointed action plan to prevent and mitigate flooding. Different class in this vulnerability map guides planner to understand which area are highly effected by flood.\nOther than SVM, this study has tried to use bivariate and multivariate methods. Apparently, SVM model performed best with 96.2% accuracy. Earlier, Mojaddadi et al. (2017) made flood vulnerability map using SVM as well for Damansara, Malaysia. The result also indicates high classification accuracy with 84.07%. Although in the summary SVM were shown has low interpretability, this study show that using the right data and combination of methods we can get high accuracy and easy to understand result.\n\n\n6.2.1.2 Drought\nnot only for flood, supervised classification also had been used to assess other natural disaster. Ghasempour, Aalami and Roushangar (2022) made Drought vulnerability maps northwest part of Iran. The study used SVM on 17 geo-environmental parameters including temperature and soil condition which are not included by Mojaddadi et al. (2017) and Youssef et al. (2023) in their researches.\n\n\n\nDrought Vulnerability Map of Northwest Iran\n\n\nsource: Ghasempour, Aalami and Roushangar (2022)"
  },
  {
    "objectID": "week06.html#reflection",
    "href": "week06.html#reflection",
    "title": "6  week_06 - Classification",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\n\nImage classification help us to identify information on imagery satellite data. It can be beneficial to learn about image classification because it can handle multiple parameter or data.\nAlthough applications of image classification are really benefiicial, there are some drawbacks on pixel-based classification. Per pixel classification does not consider similar pixel that located close ot each other. Moreover, it’s difficult to find homogeneous pixel, but in image classification, pixel value tend to be generalise\nThere are a lot of different methods on supervised and unsupervised image classification. We have to be really careful to choose the analysis methods because working with machine tempts to go with more complex model or methods. It might be true that more complex our models is, the higher accuracy we get. However, there is trade off between accuracy and interpretability, althought in the application we find that high accuracy model can also be easily understood. The takeaways is we have to aware of what kind of classification we are going to do, how is the pixel we are going to assess, and are we going to classify object or pixel. Please choos appropriate methods for your analysis.\n\n\n\n\n\n1.4. Support Vector Machines (no date). scikit-learn. Available at: https://scikit-learn/stable/modules/svm.html (Accessed: March 24, 2023).\n\n\nFisher, P. (1997) “The pixel: A snare and a delusion,” International Journal of Remote Sensing, 18(3), pp. 679–685. doi: 10.1080/014311697219015.\n\n\nGhasempour, R., Aalami, M. T. and Roushangar, K. (2022) “Drought Vulnerability Assessment Based on a Multi-criteria Integrated Approach and Application of Satellite-based Datasets,” Water Resources Management, 36(10), pp. 3839–3858. doi: 10.1007/s11269-022-03239-5.\n\n\nMojaddadi, H. et al. (2017) “Ensemble machine-learning-based geospatial approach for flood risk assessment using multi-sensor remote-sensing data and GIS,” Geomatics, Natural Hazards and Risk, 8(2), pp. 1080–1102. doi: 10.1080/19475705.2017.1294113.\n\n\nSheykhmousa, M. and Mahdianpari, M. (2020) “Support Vector Machine vs. Random Forest for Remote Sensing Image Classification: A Meta-analysis and systematic review,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. doi: 10.1109/JSTARS.2020.3026724.\n\n\nWan, Y. et al. (2017) “Evolutionary Computation Theory for Remote Sensing Image Clustering: A Survey,” in, pp. 528–539. doi: 10.1007/978-3-319-68759-9_43.\n\n\nYoussef, A. M. et al. (2023) “Flood vulnerability mapping and urban sprawl suitability using FR, LR, and SVM models,” Environmental Science and Pollution Research, 30(6), pp. 16081–16105. doi: 10.1007/s11356-022-23140-3."
  },
  {
    "objectID": "week05.html#summary",
    "href": "week05.html#summary",
    "title": "5  week_05 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this week we are going to learn about Google Earth Engine (GEE). We will discuss about GEE in 2 section. First, we are going to learn about brief introduction about GEE. Second we are going to discuss about one of processes we can do in GEE which is Reducing Image\n\n5.1.1 Google Earth Engine Introduction\nGoogle Earth Engine (GEE) is a geospatial processing server developed by Google. It allows geospatial analysis with massive raster or vector dataset that has Landsat and sentinel dataset from time to time. It allows to handle massive data store with a fast respons because all the data are stored on the server. GEE has client side and server side. Client side is what user see or frontend, meanwhile server side is the backend where GEE store all the data as Earth Engine object or identify as ee. In the GEE script, if we found anything with ee it means that it is stored in the server. Client side is place where GEE collect input from the user and after that the input will be processed in server side.\n\n\n5.1.2 Google Earth Engine Interface\nInterface of Google Earth Engine will look like this:\n\n\n\nGEE Interface\n\n\n\nAssets is where we upload our data- Documentation is documantation\nCode editor is where we put our script\nTools is tools helping navigating maps\ninspector quiery the map itself and get some information\n\n\n\n5.1.3 How Google Earth Engine works\nBefore we go further to talk about any processes we can do in Google Earth Engine, it’s better to know how GEE works. Unlike previous week and term when we use R and python, GEE is using javascript. Thus, we can not do looping like what we usually do, instead in javascript we use mapping. Mapping is making function and save it into an variable (or object) and applied to the entire collection. But, what is variable and object? in Javascript variables are container of data value written as var. Object is also variables but can contain many data values. Go check w3schools explanation on object and variable.\nIn GEE, there are 9 types or classes of objects which are shown in the image below:\n\n\n\nGEE object class\n\n\nsource: Objects in Google Earth Engine\nIt’s good to be familiar with object terminology in GEE because in the next section and next week we will use these terminology a lot. individual Landsat or satellite scene, which is a raster, is known as Image. - Image is raster data or individual satellite imagery scene - Geometry is vector or we know it as polygon - Feature is vector data with attributes (table or metadata) - Collection is stack data means that we have more than 1 (plural) data. So collection can be: - FeatureCollection which is stack or collection of vector data with attribute - ImageCollection which is stack or collection of raster image data\nThese 9 class of object also cover data manipulation which are: - Array is for multi-dimensional analysis - join is to join 2 or more dataset, and it also can be used to join imagery collection together from different sensor (such as: Landsat and sentinel) into one massive stack - Reduce is taking lots of data andsummarise into 1 point (it similar to filter or texture measure) - Reducer we will discuss this in the next section\nEach class of object has specific GEE function. The function is name of the object class with additional ee. in front of it, so for example it will look like this: - for Image use ee.Image - for image collection use ee.ImageCollection\nIn GEE, we do not need to worry about projection because by default it will be set to EPSG 3857. Only if needed (eg: if we want to load in R) we can change the projection at the end.\nAfter we understand brief information, interface and how GEE works, we can discuss further about one of methods or process that can be done in GEE. This process is Image Reducer. We will find this process beneficial because this methods helps reducing massive datasets that we are often dealt with in GEE analysis.\n\n\n5.1.4 Reducing Images\nWorking with GEE allow us to work with multi-sensor and multi temporal data. It makes us tempted to work with with massive collection of imagery data. GEE develop an image reduction tool to summarise collection of image into 1 image composite using ee.Reducer\nThere are some reduction methods we can do which are: 1. Reduce with median 2. Reduce by region 3. Reduce by Neighbourhood 4. Reduce with Linear Regression\nWe will discuss this methods one by one\n\n5.1.4.1 Reduce by Median\nThe case to use this technic is if we have lots of collection images but we only want to classify 1 image composite. Typically what GEE will do is take the median value of each pixel to make an Image composite. For instance, we want to analyse a study area in january 2023, we have image collection from several dates in January. GEE will check value of each pixel from different date and find the middle value. Afterwards, GEE will compose a new image from those median value. The median value of composed image might come from different date.\n\n\n\nReduce by median Step\n\n\nAlthough this methods commonly used (even by the lecturer, Andy), (Citation Hansen) argue to this methods of image reduction because if we only take median value it is possible to loss a lot of information. Thus, instead of using meding, he suggest to use decile.\n\n\n5.1.4.2 Reduce by Region\nOther technic of reducing image is by region. Reducing image by region can be beneficial if we have massive dataset and want to do statistic analysis on one specific study area. For instance, we want to calculate average reflectance for each band within London (the study area). What we can do is taking polygon (or using .shp file) boundaries of a region, in this case London. A region boundary possibly has a lot of pixel within. Afterwards, GEE will reduce the image by calculating mean value of each pixel within the region boundary.\nReduce by region can be done for one study area or many study area (many polygon boundaries) using: - image.reduceRegion() for 1 study polygon - image.reduceRegions() for more than 1 study polygon\n\n\n5.1.4.3 Reduce by Neigbourhood\nInstead of using region boundary, we can use Kernel neighbourhood to reduce the image. In this reducing methods is kind of similar to filtering and texture enhancement that we learnt in Chapter 3. Basically, the methods is we ask GEE to summarize the image by calculating either mean, median, min, or max value of the neighbour. Neighbourhood means, window pixels surround the central pixel. Thus, to get a value of a pixel, GEE will calculate window pixels surround it like shown in the image below. The function for reducing by neighbourhood is .reduceNeighbourhood()\n\n\n\nReduce by neirbourhood\n\n\nGEE will compose a new image from the calculation result. The Image on the left is the previous satellite image and image on the right is the composite image after reducing by neighbourhood.\n\n\n\nReduce by neirbour Result Example\n\n\nsource: GEE |  Google Earth Engine  |  Google Developers\n\n\n5.1.4.4 Reduce using Linear Regression\nBiggest benefit of GEE is we can look out data from different time so we can see how pixel values change overtime. in GEE, we can do single linear regression by using linearFit() function if we want to see the change overtime in pixel values. the function takes 2 bands which is one for dependent variable and the other is the independent variable. The observation is often time. For example, we have an image collection in January and going to see temperature change overtime. The GEE will run regression pixel by pixel with time and give us output value of the regression slope. The continues values of the slope will be mapped into new image composite. therefore, this regression technic also considered as a reduce image methods because we are reducing all the stacked data into an images. The image below is the result of linear regression. The blue color show positive slope which means the band value increase overtime. Meanwhile, the red colour show negative slope that means the value decrease overtime.\n\n\n\nLinear Regression Mapping\n\n\nsource: GEE\nOther then single linear regression, GEE can also do do Multivariate Multiple Linear Regression (MMLR) that using multiple dependent variables and independent variables. Multivariate Multiple Linear Regression is a statistical test used for more than one outcome variables using one or more independent variables (Multivariate Multiple Linear Regression, no date). The methods is similar to single linear regression that it will run pixel by pixel but for all dependent variables. The only difference is MMLR will calculate covariance matrix.\nThe case using MMLR in GEE is usually we want to calculate 2 bands for dependent variable (outcome variables). For example, we want to model pecipitation and temperature with constant variable (or control variable) and time. Each dependent variable doing its own regression pixel by pixel then it will calculate covariance matrix. Covariance is to find the value that indicates how these two variables vary together. Covariance is like variance but using 2 variables. Covariance matrix represent covariance values of each pair of variables in multivariate data (citation: 5 Things You Should Know About Covariance).\nThat was all the summary of this week material about Google Earth Engine and one of the process we can do in GEE, reduce image. There are benefits and drawbacks from using this methods. To understand better about this concept, we will see some application of this process."
  },
  {
    "objectID": "week05.html#application",
    "href": "week05.html#application",
    "title": "5  week_05 - Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\n\n5.2.1 Spatio-temporal Changes of Lake Surface Water Temperature\nCloud-based storage system of GEE allow us to do massive analysis with hundreds of images. This can be beneficial if we want to do study about spatio temporal changes because in GEE we can access data from multiple sensor and temporal. Albarqouni (2022), use GEE to analysis long-term spatio-temporal changes of lake surface water temperature (LSWT). To see change, this study includes 606 images from 2000 to 2021 from Landsat 5 TM and Landsat 8 OLI. 2 image sources were needed because Landsat 5 TM only collect data until 2011; thus we need Landsat 8 OLI to acquire data in 2013-2021. To calculate Water Surface Area Extraction, multispectral bands of these image collections was applied to normalized difference water index (NDWI). To measure LSWT, thermal bands was utilised to calculate Land Surface Emissivity. Then, relationship of LSWT and water surface area will be assessed. Since this analysis was conducted in GEE, the image collection was stored in the cloud.\n\n\n\nLSWT analysis flowchart\n\n\nsource: Albarqouni et al. (2022)\n\n\n5.2.2 Forest Canopy Closure\nXie et al. (2022) takes advantages of GEE to monitoring forest changes. This study determining vegetation and bare soil endmember the normalized differences vegetation index (NDVI), modified bare soil index (MBSI), and bare soil index (BSI). This study also utilising GEE’s abilities of conducting massive analysis and providing data from various satellite. To help handle big dataset, this studyproduce image composite by reducing topographic-corrected cloudless using median reducer to pick median value in each band, in each pixel, overtime. Using image composite helps the analysis process becase it summarise the image collection into 1 image composite. This study tested the proposed methods using Landsat 8 and Sentinel-2 data. The result show that the proposed method is robust and reproducible using different sensor but with similar wavebands.\n\n\n\nForest Canopy Closure Change\n\n\nsource: Xie et al. (2022)\nWhat we can learn from this application is, various satellite data given by GEE can be combined into one analysis like what we see in LSWT research. Also, it can be treated as different input to compare the model like what we see in Forest Canopy Closure. in GEE it will be easy to access data from different tima and sensor. Although we can get image collection easily, we have to be critical to image that we get. For example, do our images require atmospheric correction? Especially regarding this example, estevezGaussianProcessesRetrieval2022, succesfully avoided atmospheric correction by assessing top-of-atmspher (TOA)"
  },
  {
    "objectID": "week05.html#reflection",
    "href": "week05.html#reflection",
    "title": "5  week_05 - Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\n\nGEE store their datasets on clouds which makes it not require any local storage and allows to handle massive analysis with huge amount of data from multiple spectral, temporal, and sensor. We can see change with multitemporal data. We can compare the model with different input data. Ease of accessing various data can be overwhelming and prone to over-complicating, Therefore, we have to be critical on what we want to study and what kind of data that appropriate enough for the analysis.\nSince the data stored on the cloud, GEE allow us to work from anywhere. It can fit the idea of equity and justice because in developing country might does not have much geo-spatial talents. Because of the possibility to work remotely, a project team can include team member from different country\nMoreover, working with GEE we can access alot of image scene without spending anymoney. It can be beneficial for low-budget team or governemnt to save some money and alocated their money on project implementation instead.\n\n\n\n\n\nAlbarqouni, M. M. Y. et al. (2022) “Assessment of Spatio-Temporal Changes in Water Surface Extents and Lake Surface Temperatures Using Google Earth Engine for Lakes Region, Türkiye,” ISPRS International Journal of Geo-Information, 11(7, 7), p. 407. doi: 10.3390/ijgi11070407.\n\n\nMultivariate Multiple Linear Regression (no date). StatsTest.com. Available at: https://www.statstest.com/multivariate-multiple-linear-regression/ (Accessed: March 19, 2023).\n\n\nXie, B. et al. (2022) “Improved Forest Canopy Closure Estimation Using Multispectral Satellite Imagery within Google Earth Engine,” Remote Sensing, 14(9, 9), p. 2051. doi: 10.3390/rs14092051."
  },
  {
    "objectID": "week03.html#summary",
    "href": "week03.html#summary",
    "title": "3  week03 - Remote Sensing Data",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nIn this week we learn about history of Landsat data and pre-processing of imagery satellite data.\nWe have to thank Virginia Wood for the landsat data we have nowadays because previously NASA wanted to use analogue system (RBV camera - TV camera with Green, Red and NIR spectrum) for their sensor. This is the spectral that catch by the Landsat RBV, and then cutted into only 4 bands, green, red, red-near IR, and near-IR. Then, Virginia Norwood suggested to use Multispectal Camera (MSS) which allows us to have 7 bands image. This became standar for the following landsat. Since then she is known as “the Mother of Landsat”\nPreprocessing step is a step we do to make our imagery data ready to be processed (classified) or analysed. It can be seen on the mind map that preprocessing steps we can do 3 thing, which are: 1. Correction 2. Data Joining 3. Enhancement Not every imagery data required all these 3 steps, it depends on the data we get and what we are going to do. Check this mind map to understand the outline of pre-processing steps.\n\n3.1.1 Correction\n\nPre-processings are required (occasionally) because imagery data can contain flaws or errors from the sensor, atmosphere, terrain and more\nScan Line Correction (SLC) is pair of mirror to compansate the forward movement of satellite so the resulting scan are shown paralel\n\n\n\n\nGEE Interface\n\n\nFailed scan line landsat is on Landsat 7, because it moves in a zig zag, and the corrector made the image normal\nImagery was still distributed but it is hard to use with methods developed to estimates the gaps, termed gap filling.\n\n\n\n3.1.1.1 1. Geometric Correction\n\nMeans geometry on how the image located on earth.\nSatelite image has CRS (Coordinate Reference System)\nImage distortions can be introduce due to:\n\nView angle (off-nadir)* - Nadir means directly down\n\n\n\n\nView Angle\n\n\n  - if the satelite off nadir, it will cause shadowing  and we have to correct it\n\nTopography (e.g. hills not flat ground)\nWind (if from a plane)\nRotation of the earth (from satellite)\n\nbecause of the earth rotation, the image produced will be off grid, so it has to be aligned\n\n\n\n\n\n3.1.1.1.1 Geometric Correction Solution\n\n\n\n\nGeometric correction solution\n\n\nSteps:\n\nidentify Ground Control Points (GCP) to match known points in the image and a reference dataset\n\nReference Dataset can be:\n\nLocal Map\nAnother image\nGPS data from handheld device\n\nGCP typically using object that does not move, such as:\n\nParking lot\nBuilding\n(typically non-vegetation)\n\n\nTake the coordinates and the model them to give geometric transformation coefficients\nTransform the GCP coordinates to the right one using linear regressrion\nplot these and try to minimise the RMSE\n\nModelling\n\nForward mapping\n\npredicting corrected image with uncorrected image\n\n\n\nForward Mapping formula\n\n\n x and y are positions in the corrected map\nBut the issue is that we are modelling the rectified x and y which could fall anywhere on the gold standard map (e.g. not on a grid square or at a floating point)\nforward mapping isn’t the most common one to use\n\n\n\nForward Mapping\n\n\n\nBackward Mapping\n\nPredicting the uncorrected image with the corrected image\n\n\n\nBackward Mapping\n\n\nEvery value in the output (corrected image) pixel will have value in the original input (uncorrected) image.\nthe images are distorted so might not completely overlap. The goal is to match distorted image with gold standard image, so we want the pixel to line up\n\n\n\nBackward Mapping\n\n\n\n\nResampling\n\nresampling is transforming from grid to another grid\nwe need to do re-sampling because the image data we get might slightly shifted. ANd is can be useful if the image has different grid size (or different band that has different resolution, like in the 1st week practical)\n\n\n\n\n\n\n3.1.1.2 2. Atmospheric Correction\n\nAtmospheric correction is the influence of atmospher on our data:\n\nAtmospheric scattering\ntopography Attenuation (reduction)\n\nThe goal is to remove the influence of atmosphere\nSituation where necessary or unnecessary to do atmospheric correction:\n\nUnnecessary\n\nif just look into one single images, because we dont have to see data across time\n\nNecessary\n\ntypically if we have time constrain. To compare data in multiple time stamp\n\n\nScattering create haze that reduce the contrast pf the image\nbright reflective material, eg, concrete, asphalt. karena terlalu terang jadi bocor ke sekitarnya\n\n\n3.1.1.2.1 Atmospheric Correction Solution\n\nRelative\n\nAdjust some data relative to something else as reference\nType\n\nDark object subtraction (DOS)\n\nDone by searching dark value (usually water) of each band and substract that value from each pixel\n\nPsuedo-invariant Features (PIFs)\n\nthis used when we have may images. We pick 1 image as based image. Determine feature that don’t change. Make regression model with Y is the based image. Adjust the ijmage based on regression model\n Pseudo invariant feature\n\n\n\nAbsolute\n\nChange digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet\nBasically made atmospheric model called atmospheric radiative transfer models\n\n\n\n\n\n3.1.1.3 3. Topography Correction/ Orthorectification Correction\n\nit means when we are not looking straight down (nadir), so the image distorted.\northorectification means removing distortion by making the pixel viewed at nadir\nfor orthorectification we need sun zenith and azimut angle, and orientation of the slope from DEM.\n\n\n\nZenith Azimut ilustration\n\n\n\nZenith meas directly up while nadir means directly down\nAzimut is position of sun to north, south, east, west\n\nAtmospheric typically happen before topographic correction\n\n\n\n3.1.1.4 4. Radiometric Calibration\n\nSensor capture image as Digital Number with no units. Spectral Radiance is the amount of light within a  band from a sensor in the field of view\nRadiometric Calibration: Calibrate the data (digital Number) into radiance and convert to specific unit\n\nRadiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor\n\nbasically, before sending sensor to the space, calibaration was done to check whether sensors performing correctly or not. We then use the calibration measurements to adjust the data captured by the sensor\n\n\n\n\n\n3.1.2 Joining Datasets/Enhancement\n\nthe ideas is joining or merging or mosaicking or feathering images into one seamless image\nThis process was done by taking few pixel from each image at the same location and overlaping those pixels. Then, blending these to image on the ovelaped pixels\nThose image are ovelapping by 20-30%\n source Seamless Mosaic (l3harrisgeospatial.com)\n\n\nsource gdal - How to create a mosaic in QGIS with cutline and feathering for Landsat-8 imagery - Geographic Information Systems Stack Exchange - However the challenge is the image we get might coming from different day and lighting condition or even different satellite. It can cause different band value thus those image have to undergo standarisation and normalisation - Standarisation by dividing the SR value by a maximum value per band - normalisation by divide the standarised value by the sum of values across all bands\n\n\n3.1.3 Image Enhancement\n\nit doesn’t alter the value of the data, merely changing how it explains and visual appearance\n\n\n3.1.3.1 Contrast Enhancement\n\n\n\n\ncontrats enhancemenct\n\n\ndone by:\n\nstretching min max value\npercentage Linear and Standar Deviation\nPiecewise Linear Contrast Stretch\n\nit doesn’t alter the value of the data, merely changing how it explains and visual appearance\n\n\n\n3.1.3.2 Ratio enhancement\n\nband ratioing means dividing bands by each other\neg: Normalize Burn Ratio\n\n\n\n\nratio enhancement\n\n\nsource: Landsat Normalized Burn Ratio | U.S. Geological Survey (usgs.gov)\n\n\n\n\n3.1.3.3 Filtering\n\nmeans taking an image an having a moving window to see aggregation of the image. Calculate surround pixel and put the average value as the middle pixel’ value\nLow Pass or low frequency is average of the surrounding pixels\nHigh pass or high frequency is enhance local vatiations\n\n\n\n\n3.1.3.4 Principal Component Analysis\n\nUsing PCA we can make our data smaller and maximise variation between our data\nPCA will transforming multi-spectral data into uncorrelated and smaller data set\nReduce dimensionality\nExample:\n\nmulti-temporal PCA bands from both time points are combined into one image, then PCA\n\n\n\nPCA\n\n\nso initially there are 2 or 3 images from different time stamps. It was stacked together and did PCA. From the PCA,they classify land use changes. Then maximise the variation.\n\n\n\n\n3.1.3.5 Texture Enhancement\n\nTextture is spatial variation of gray value\nusually used for medical detection\nTexture analysis looks at the tonal feature of the image by looking at the surrounding values. So there’s a moving window with 3x3 grid and it will calculate the value of a pixel with variance and probability of surrounding (within the window) value\nTexture variance result\n\n\n\n\nTexture Enhancement\n\n\nsisi yang terang adalah pinggir2 gedung karena tepi texture has high variance value\n\nTexture is beneficial to give additional information to our model as it oppose to just relying on spectral reflectance. Thus we can improve our classification model.\n\n\n\n3.1.3.6 Data Fusion enhancement\n\nstack of multiband data fused with PCA or texture or other enhancement.\nImage fusion can alse be from different sensor. eg. Sentinel fused with Landsat\nusually it take the median value of each pixel of each band."
  },
  {
    "objectID": "week03.html#application",
    "href": "week03.html#application",
    "title": "3  week03 - Remote Sensing Data",
    "section": "3.2 Application",
    "text": "3.2 Application\nLearning about image enhancement in pre-processing step are really beneficial especially when we are dealing with huge datasets, such as hyperspectral and multi-temporal imagery (Rodarmel and Shan, 2002). Principal Component Analysis enhancement is a tools to reducing dimensionality if we are detecting land-use change from time to time using stacked multi-date data like what Deng (2008) in their paper. In detecting land-use change in Hangzhou City, China, they also use data from various sensors which are aerial photograph, SPOT-5 and Landsat-7. !\nThe image above shows one example of land use change from cropland to urban land. These image shows:\na ETM in 2000\nb aerial photograph in 2000\nc SPOT-5 in 2003\nd Ikonos in 2003\ne-h are the principal component.\nsource: Deng (2008) Using PCA wil make it easier to see detect the change because it will produce a new image (Principal component) that intensify the change (Ingebritsen and Lyon, 1985). In Deng’s paper, PCA was used to combining image band that taken from two different times into one new image. Changed area will have high correlation between two image, meanwhile unchanged are will have low correlation. Afterwards Deng classified and labelled the correlation value to detect changing area.\nIn this analysis the usage of PCA on multitemporal and multisensor data shows high accuracy value which is 89.54%. Other application of PCA in hyperspectral done by Rodarmel (2002), also shows satisfying result with 70% correct classification rate. Rodarmel and Shan, use hyperspectral data to detect component of land cover, such as, vegetation, mineral and soil type. They stacked band 1-5, 1-10 and 1-25 from HYDICE image and use PCA to generate 4 different PC images. Afterwards they classified the result of each PCA image and compared with original image PCA. The result shows PCA from band 1-5 is contain most information, while bands beyond 10 only contain noise.\n\n\n\nFlood in Semarang\n\n\nsource: Principal Component Analysis for Hyperspectral Image Classification\nAlthough, PCA helps reducing image dimensionality, the output does not give much information. It has to be processed using image classification, as shown in the 2 paper I discussed above (Licciardi et al., 2012). We can say that output of PCA is input for other analysis. Thus, to do a robust analysis, we also need to learn about methods other than image enhancement."
  },
  {
    "objectID": "week03.html#reflection",
    "href": "week03.html#reflection",
    "title": "3  week03 - Remote Sensing Data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis week content about image correction, data joining and enhancement are really interesting because it is beneficial in practical and academical context. Imagery data that we get from sensor is not always perfect, it might have some error that come from the atmospher or the sensor itself. Especially in some region with high degree of moist, acquiring clear image is a challenge due to the existance of cloud (Deng et al., 2008). This can be tackled by atmospheric correction or if we use multitemporal data we can do PCA. Not only from external factors, but image error can also happend because the radiometric calibration of the sensor. Thus, to conduct an accurate and robust analysis we have to reduce the error by doing corrections.\nImage enhancement helps to handle huge data set from multi-temporal, multispectral, and multi-sensor images. This is beneficial to see the change of land use. For developing countries, where development happen organically, a lot of residential built without permission so the government does not have the data about land usage in the city. By using remote sensing to detect the land-use changing will help government to be aware of the informal residential and observe where informal development tend to happen over time.\nHowever, we cannot just stop at learning about image enhancement because it only give us input for further analysis. We should also learn about image classification to detect land use and land coverage. For classification methods you can go to chapter 6 and 7.\n\n\n\n\nDeng, J. S. et al. (2008) “PCA‐based land‐use change detection and analysis using multitemporal and multisensor satellite data,” International Journal of Remote Sensing, 29(16), pp. 4823–4838. doi: 10.1080/01431160801950162.\n\n\nIngebritsen, S. E. and Lyon, R. J. P. (1985) “Principal components analysis of multitemporal image pairs,” International Journal of Remote Sensing, 6(5), pp. 687–696. doi: 10.1080/01431168508948491.\n\n\nLicciardi, G. et al. (2012) “Linear Versus Nonlinear PCA for the Classification of Hyperspectral Data Based on the Extended Morphological Profiles,” IEEE Geoscience and Remote Sensing Letters, 9(3), pp. 447–451. doi: 10.1109/LGRS.2011.2172185.\n\n\nRodarmel, C. and Shan, J. (2002) “Principal Component Analysis for Hyperspectral Image Classification,” Surv Land inf Syst, 62."
  }
]