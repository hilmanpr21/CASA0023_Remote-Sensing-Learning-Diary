[
  {
    "objectID": "week01.html#summary",
    "href": "week01.html#summary",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote sensing is acquiring data or information from a distance. In this module, context we use remote sensing for Earth Observation (EO). This can be done by using sensor mounted on satelite.\nIn this week we will talk general topic about sensor and satellite on on earth observation. It can be seen in the mind map that there will be 4 subtopics to make us easier understanding Sensor, which are:\n\nTypes of Sensor\n\nPassive Sensor\nActive Sensor\n\nElectromagnetic waves or radiation (EMR)\n\nEMR interaction with atmosphere before hitting sensor\nEMR interaction with earth surface before hitting sensor\n\nType or satellite orbit\n\nGeosynchronous Orbit\nGeostationary Orbit\n\nData produced by sensor\n\nRaster Data Format\nResolution\n\nSpectral\nSpatial\nTemporal\nRadiometric\n\n\n\nCheck the mind map to understand the outline of this week’s topic\n\n\n\nMind map\n\n\n\n1.1.1 Types of Sensor\n\n1.1.1.1 Passive Sensor\n\nUse electromagnetic waves from sun-ray as source of energy.\nDetect reflected energy from the sun that has different wavelength to detect different colour spectrum\nSuch as:\n\nsentinel\nlandsat \n\n\n\n\n1.1.1.2 Active Sensor\n\nHave an energy source for Illumination\nActively emits electromagnetics wave and then wait to rceive\nSuch as:\n\nSynthetic-aperture Radar (SAR),\nX-ray,\nLiDAR\n\n\n\n\nActive Sensor\n\n\n\n\n\n\n1.1.2 Electro Magnetic waves\n\nis waves that have both eletric and magentic field, produce by vibration of particles\nEM waves move in vacuum\nPerpendicular electric and magnetic field\ndifferent wavelength causes different colour spectrum. That’s why we can see colourful image\n\n\n\n\nElectromagnetive Wave\n\n\n\n1.1.2.1 Terms\n\nElectromagnetic radiation (EMR) = Waves of an electromagnetic field, travel through space and carry radiant energy\nRadiant Energy = energy carried by EMR Waves\nRadian flux = Energy per unit of time\nExitance emittance (per unit time\nflux) = energy leaving a surface per unit area per unit time - Flux = time\n\n\n\n1.1.2.2 Electromagnetric Radiaton (EMR) interaction\n\nBefore hitting hitting the sensor, energy from electromagnetic radiation will interacting with:\n\natmosphere\n\nscattered by atmosphere particle\n\nearth’s surface\n\nabsorbed\ntransmitted\n\n\n\n\n1.1.2.2.1 Atmospheric Scattering\n\nElectromagnetic waves are scatters by particles on the atmosphere \nSmaller wavelengths scatter easier\nBlue has the smallest wavelength \ntype of scattering:\n\nRayleigh is when particles are very small compared to the wavelength.\n\neg: sky looks blue because sunray scattered by particles. in sunset or sunrise is much more atmosphere to pass throught so more scattered. When sun’s angle change the blue light scatter does not reach our eyes as the distance is increased, so longer wavelength like red and orange reach us\n\nMie is when particles are same size compared to the wavelength\nNon Selective is when particles are much larger compared to the wavelength\n\ndrawback of passive sensor is that it could not pass through cloud\n\n\n\n1.1.2.2.2 Bidirectional Documented Surface Interaction (BDRF)\n\nhappens because sensor and illumination (sun) angles can change.\nIt can cause\n\nbackscattering (left): sun behind the sensor, bright region at sensor and sun side\nforwards scattering (right): sun opposite satelite, shadow region at sensor side\n\n\n\n\nBack Scattering and Forward Scattering\n\n\n\n\n“There are many interaction that influence the data being created because it’s not merely say that energy from sun is reflected by earth to sensor (but there are may factor such as scattering and interaction with surface)”\n\n\n\n\n\n1.1.3 Types of orbit\n\nGeosynchronous Orbit (GSO)\n\nsatelite matches the earth’s rotation, usually for sensor\n\nGeostationary Orbit\n\nsatelite holds same position, usually only for communication\n\n\n\n\nOrbit type\n\n\n\n\n\n1.1.4 Data\n\n1.1.4.1 Raster Format\n\nSatellite sensor produces raster imagery data. Raster image is a graphic repesented as a rectangular matrix or grid of square pixels.\n\n\n\n\nRaster Grid\n\n\n\n\n1.1.4.2 Four Data Resolution\n\n1.1.4.2.1 Spatial\n\nSpatial resolution is the size of the raster grid per pixel. The smaller the grid means the higher the resolution be so the clearer the image be\n\nsmallest grid is 10cm\n\n\n\n\n1.1.4.2.2 Temporal\n\nTemporal resolution is showing how often the data being updated\n\n\n\n1.1.4.2.3 Radiometric\n\nRadiometric is identified differences in light or reflectance, in practice this is the range of possible values. or the ability od a sensor to identify and show small difference in energy.\nThe higher the bit, the more sensitive and the more information\nhowever, the lower the radiometric resolution, the higher possibility to differentiate features\n\n\n\n1.1.4.2.4 Spectral\n\nthis concept similar to pixel concept in digital camera where different colour (called band) is stacked together so it makes a new colour combination.\n\n\n\n\npixel band system\n\n\nsource: The hype in spectral imaging | Spectroscopy Europe/World\n\nspectral resolution based on number of bands being observed\n\n - We can take values for each wavelength (or a band of several wavelengths) across the electromagnetic spectrum to create a spectral signature. different feature on earth has different spectral signature, it will be used to identify different object. spectral signature can be discrete (eg. multispectral) or continuous (eg. hyperspectral)  - This spectral signature are used to differentiate object in the image. In earth observation we use this for identifying land cover in earth surface using Normalized Difference Vegetation Index (NDVI), Normalized Difference Building Index (NDBI) - for example in this week practical we identify the landcover by making scatter plot with band 4 (red, vegetation absorb) and band 8 (Near-Infrared, NIR, that vegetation strongly reflect). high values of NIR and low values of red represents dense vegetation. Meanwhile low values of red and NIR represent dense vegetation"
  },
  {
    "objectID": "week01.html#application",
    "href": "week01.html#application",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application\nThe increasing satellite remote sensing data on earth observation in the last two decades, allows new approaches on understanding urban setting (Miller and Small, 2003). Satellite remote sensing data offers some advantages, such as: - broader spatial coverage, - ability for routine update, - consistent measurement\nCombination of remote sensing data with census data or other government data can provide us useful information such as, urban growth, urban heat island, and socioeconomic activities. We will discuss some application examples of remote sensing data, classified based on sensor type.\n\n1.2.1 Active Sensor\n\n1.2.1.1 Population Size estimation using Synthetic Aperture Radar (SAR)\nDifferent from passive sensor, Synthetic Aperture Radar (SAR) is an active sensor that can see through cloud and the backscatter radar wave can determine physical properties such as density and surface texture (Henderson and Zong-Guo Xia, Jan./1997). The ability of SAR to detect horizontal and vertical structure allows to detect settlement based on texture, materials geometry and density. The result of settlement detection can be used for estimating city population by multiplying with number of occupants per dwelling. Harris (1985) used this methods for estimating population in Tunisia.\nThis methods is beneficial for developing countries because they have more informal types of dwellings. Radar Data is really helpful because population census happens only once in 10 years (Henderson and Zong-Guo Xia, Jan./1997) which could not keep up the rapid urban growth.\n\n\n\n1.2.2 Passive Sensor\n\n1.2.2.1 Evaluating Poverty using Nighttime Light (NTL) Satellite\nNighttime Light Satellite is a passive sensor launched by nasa to collect data of night light emission (Earth Science Data Systems, 2021). The data from this satellite provide unique perspective that can be beneficial for detecting global conflict, human night behaviour, ecological impact of artificial night light and so on.\nOne of the benefit of nighttime light data is to track socioeconomic inequality and poverty. Yu (2015) evaluated poverty rate in 2856 counties in china. The research compare the average light index from Visible Infrared day-night band with integrated poverty index. It used linear regression and comparison of class to check how accurate the remote sensing data to evaluate poverty. The result shows that day-night band data can be useful to evaluate poverty in china"
  },
  {
    "objectID": "week01.html#reflection",
    "href": "week01.html#reflection",
    "title": "1  week 01 - Remote Sensing Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nThe increase of remote sensing data on earth observation helps researcher, policy makers, and local government to understand the urban context better. Special characteristic of remote sensing data allows us to have broader perspectives and collect spatial improvement from time to time which could not be provided by tradisional population census data.\nIn the application we can combine the remote sensing data with census data to have more robust analysis.\nAlthough, using remote sensing data looks more fancy, in some cases and context we can get the similar data that are more efficient and cost effective from governmental source (Miller and Small, 2003). For example we can use building permission data rather than satellite data to identify land use.\nMoreover, managing remote sensing data require higher skill than traditional data. It can be an obstacle for cities that has limited resources\n\n\n\n\n\nEarth Science Data Systems, N. (2021) Nighttime Lights. Earthdata; Earth Science Data Systems, NASA. Available at: https://www.earthdata.nasa.gov/learn/backgrounders/nighttime-lights (Accessed: February 15, 2023).\n\n\nHarris, R. (1985) “SIR-A imagery of Tunisia and its potential for population estimation,” International Journal of Remote Sensing, 6(7), pp. 975–978. doi: 10.1080/01431168508948260.\n\n\nHenderson, F. M. and Zong-Guo Xia (Jan./1997) “SAR applications in human settlement detection, population estimation and urban land use pattern analysis: A status report,” IEEE Transactions on Geoscience and Remote Sensing, 35(1), pp. 79–85. doi: 10.1109/36.551936.\n\n\nMiller, R. B. and Small, C. (2003) “Cities from space: Potential applications of remote sensing in urban environmental research and policy,” Environmental Science & Policy, 6(2), pp. 129–137. doi: 10.1016/S1462-9011(03)00002-9.\n\n\nYu, B. et al. (2015) “Poverty Evaluation Using NPP-VIIRS Nighttime Light Composite Data at the County Level in China,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 8(3), pp. 1217–1229. doi: 10.1109/JSTARS.2015.2399416."
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  week09",
    "section": "",
    "text": "Hai lalalala"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "9  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RS_Learning_diary",
    "section": "",
    "text": "Welcome to my learning diaries\nHi, welcome to my portfolio. It’s my weekly learning diaries that hopefully can help me recalling what I have learn in CASA0023 Remotely Sensing Cities and Environments module"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "RS_Learning_diary",
    "section": "Introduction",
    "text": "Introduction\nShort introduction about me.\nI am Hilman Prakoso, an Indonesian living in London. Currently I am a Master of Science candidate in Urban Spatial Science Program in University College London. In my undergaduate, I studied architecture in a four-year program at Institute Technology Bandung.\nMy interest are related to socio-spatial analysis, spatial data visualisation, and informal settlement mapping. Along with 2 other friends, I built an urban data centre platrform for my hometown, Surabaya, called Suroboyo Ngalor Ngidul. This platform aims to collect and spread data about Surabaya, especially in urban mobilities, social, and economy."
  },
  {
    "objectID": "index.html#about-this-learning-diaries",
    "href": "index.html#about-this-learning-diaries",
    "title": "RS_Learning_diary",
    "section": "About this Learning Diaries",
    "text": "About this Learning Diaries\nEvery chapter discusses different topics which generally structured into 3 segments, summary, application, and reflection. Some topics will have different structure due to specific question to answer.\nRunning through this learning diaries will help me and you to understand what remote sensing is and has broad ideas of what we can do with earth observation in city planning"
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  week02",
    "section": "",
    "text": "In this week we learn about one of the sensor. But instead of having it in normal diary format, we want to try idfferent format, Xaringan presentation.\n\n\n\n\n\n\n\n\nTo view this presentation full-screen, click here."
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  week03",
    "section": "",
    "text": "4 Summary\nIn this week we learn about history of Landsat data and pre-processing of imagery satellite data.\nWe have to thank Virginia Wood for the landsat data we have nowadays because previously NASA wanted to use analogue system (RBV camera - TV camera with Green, Red and NIR spectrum) for their sensor. This is the spectral that catch by the Landsat RBV, and then cutted into only 4 bands, green, red, red-near IR, and near-IR. Then, Virginia Norwood suggested to use Multispectal Camera (MSS) which allows us to have 7 bands image. This became standar for the following landsat. Since then she is known as “the Mother of Landsat”\nPreprocessing step is a step we do to make our imagery data ready to be processed (classified) or analysed. It can be seen on the mind map that preprocessing steps we can do 3 thing, which are: 1. Correction 2. Data Joining 3. Enhancement Not every imagery data required all these 3 steps, it depends on the data we get and what we are going to do. Check this mind map to understand the outline of pre-processing steps.\nLearning about image enhancement in pre-processing step are really beneficial especially when we are dealing with huge datasets, such as hyperspectral and multi-temporal imagery (Rodarmel and Shan, 2002). Principal Component Analysis enhancement is a tools to reducing dimensionality if we are detecting land-use change from time to time using stacked multi-date data like what Deng, (2008) in their paper. In detecting land-use change in Hangzhou City, China, they also use data from various sensors which are aerial photograph, SPOT-5 and Landsat-7. [[Pasted image 20230315165944.png]] The image above shows one example of land use change from cropland to urban land. These image shows:\na ETM in 2000\nb aerial photograph in 2000\nc SPOT-5 in 2003\nd Ikonos in 2003\ne-h are the principal component.\nsource: Deng, (2008) Using PCA wil make it easier to see detect the change because it will produce a new image (Principal component) that intensify the change (Ingebritsen and Lyon, 1985). In Deng’s paper, PCA was used to combining image band that taken from two different times into one new image. Changed area will have high correlation between two image, meanwhile unchanged are will have low correlation. Afterwards Deng classified and labelled the correaltion value to detect changing area.\nIn this analysis the usage of PCA on multitemporal and multisensor data shows high accuracy value which is 89.54%. Other application of PCA in hyperspectral done by Rodarmel, (2002) also shows satisfying result with 70% correct classification rate. Rodarmel and Shan, use hyperspectral data to detect conponent of land cover, such as, vegetation, mineral and soil type. They stacked band 1-5, 1-10 and 1-25 from HYDICE image and use PCA to generate 4 different PC images. Afterwards they classified the result of each PCA image and compared with original image PCA. The result shows PCA from band 1-5 is contain most information, while bands beyond 10 only contain noise. [[Pasted image 20230315180749.png]] source: Principal Component Analysis for Hyperspectral Image Classification\nAlthough, PCA helps reducing image dimensionality, the output does not give much information. It has to be processed using image classification, as shown in the 2 paper I discussed above (Licciardi et al., 2012). We can say that output of PCA is input for other analysis. Thus, to do a robust analysis, we also need to learn about methods other than image enhancement.\nThis week content about image correction, data joining and enhancement are really interesting because it is beneficial in practical and academical context. Imagery data that we get from sensor is not always perfect, it might have some error that come from the atmospher or the sensor itself. Especially in some region with high degree of moist, acquiring clear image is a challenge due to the existance of cloud (Deng et al., 2008). This can be tackled by atmospheric correction or if we use multitemporal data we can do PCA. Not only from external factors, but image error can also happend because the radiometric calibration of the sensor. Thus, to conduct an accurate and robust analysis we have to reduce the error by doing corrections.\nImage enhancement helps to handle huge data set from multi-temporal, multispectral, and multi-sensor images. This is beneficial to see the change of land use. For developing countries, where development happen organically, a lot of residential built without permission so the government does not have the data about land usage in the city. By using remote sensing to detect the land-use changing will help government to be aware of the informal residential and observe where informal development tend to happen over time.\nHowever, we cannot just stop at learning about image enhancement because it only give us input for further analysis. We should also learn about image classification to detect land use and land coverage. For classification methods you can go to chapter 6 and 7."
  },
  {
    "objectID": "week03.html#correction",
    "href": "week03.html#correction",
    "title": "3  week03",
    "section": "4.1 Correction",
    "text": "4.1 Correction\n\nPre-processings are required (occasionally) because imagery data can contain flaws or errors from the sensor, atmosphere, terrain and more\nScan Line Correction (SLC) is pair of mirror to compansate the forward movement of satellite so the resulting scan are shown paralel\n\n[[Pasted image 20230124131421.png]]\nFailed scan line landsat is on Landsat 7, because it moves in a zig zag, and the corrector made the image normal\nImagery was still distributed but it is hard to use with methods developed to estimates the gaps, termed gap filling. ### 1. Geometric Correction\n\nMeans geometry on how the image located on earth.\nSatelite image has CRS (Coordinate Reference System)\nImage distortions can be introduce due to:\n\nView angle (off-nadir)* - Nadir means directly down\n\n[[Pasted image 20230220161521.png]]\nif the satelite off nadir, it will cause shadowing and we have to correct it\n\nTopography (e.g. hills not flat ground)\nWind (if from a plane)\nRotation of the earth (from satellite)\n\nbecause of the earth rotation, the image produced will be off grid, so it has to be aligned\n[[Pasted image 20230222070839.png]] #### Geometric Correction Solution\n\n\n[[Pasted image 20230220161558.png]]\nSteps:\n\nidentify Ground Control Points (GCP) to match known points in the image and a reference dataset\n\nReference Dataset can be:\n\nLocal Map\nAnother image\nGPS data from handheld device\n\nGCP typically using object that does not move, such as:\n\nParking lot\nBuilding\n(typically non-vegetation)\n\n\nTake the coordinates and the model them to give geometric transformation coefficients\nTransform the GCP coordinates to the right one using linear regressrion\nplot these and try to minimise the RMSE\n\nModelling\n\nForward mapping\n\npredicting corrected image with uncorrected image\n[[Pasted image 20230228102446.png]]\n x and y are positions in the corrected map\nBut the issue is that we are modelling the rectified x and y which could fall anywhere on the gold standard map (e.g. not on a grid square or at a floating point)\nforward mapping isn’t the most common one to use\n[[Pasted image 20230220165826.png]]\n\nBackward Mapping\n\nPredicting the uncorrected image with the corrected image\n[[Pasted image 20230228102853.png]]\nEvery value in the output (corrected image) pixel will have value in the original input (uncorrected) image.\nthe images are distorted so might not completely overlap. The goal is to match distorted image with gold standard image, so we want the pixel to line up\n[[Pasted image 20230220165951.png]]\n\n\nResampling\n\nresampling is transforming from grid to another grid\nwe need to do re-sampling because the image data we get might slightly shifted. ANd is can be useful if the image has different grid size (or different band that has different resolution, like in the 1st week practical)\n[[Pasted image 20230228104428.png]]\n\n\n\n4.1.1 2. Atmospheric Correction\n\nAtmospheric correction is the influence of atmospher on our data:\n\nAtmospheric scattering\ntopography Attenuation (reduction)\n\nThe goal is to remove the influence of atmosphere\nSituation where necessary or unnecessary to do atmospheric correction:\n\nUnnecessary\n\nif just look into one single images, because we dont have to see data across time\n\nNecessary\n\ntypically if we have time constrain. To compare data in multiple time stamp\n\n\nScattering create haze that reduce the contrast pf the image\nbright reflective material, eg, concrete, asphalt. karena terlalu terang jadi bocor ke sekitarnya\n\n\n4.1.1.1 Atmospheric Correction Solution\n\nRelative\n\nAdjust some data relative to something else as reference\nType\n\nDark object subtraction (DOS)\n\nDone by searching dark value (usually water) of each band and substract that value from each pixel\n\nPsuedo-invariant Features (PIFs)\n\nthis used when we have may images. We pick 1 image as based image. Determine feature that don’t change. Make regression model with Y is the based image. Adjust the ijmage based on regression model\n[[Pasted image 20230228111657.png]]\n\n\n\nAbsolute\n\nChange digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet\nBasically made atmospheric model called atmospheric radiative transfer models\n\n\n\n\n\n4.1.2 3. Topography Correction/ Orthorectification Correction\n\nit means when we are not looking straight down (nadir), so the image distorted.\northorectification means removing distortion by making the pixel viewed at nadir\nfor orthorectification we need sun zenith and azimut angle, and orientation of the slope from DEM.\n[[Pasted image 20230228113746.png]]\n\nZenith meas directly up while nadir means directly down\nAzimut is position of sun to north, south, east, west\n\nAtmospheric typically happen before topographic correction\n\n\n\n4.1.3 4. Radiometric Calibration\n\nSensor capture image as Digital Number with no units. Spectral Radiance is the amount of light within a  band from a sensor in the field of view\nRadiometric Calibration: Calibrate the data (digital Number) into radiance and convert to specific unit\n\nRadiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor\n\nbasically, before sending sensor to the space, calibaration was done to check whether sensors performing correctly or not. We then use the calibration measurements to adjust the data captured by the sensor\n[[Pasted image 20230228115429.png]]"
  },
  {
    "objectID": "week03.html#joining-datasetsenhancement",
    "href": "week03.html#joining-datasetsenhancement",
    "title": "3  week03",
    "section": "4.2 Joining Datasets/Enhancement",
    "text": "4.2 Joining Datasets/Enhancement\n\nthe ideas is joining or merging or mosaicking or feathering images into one seamless image\nThis process was done by taking few pixel from each image at the same location and overlaping those pixels. Then, blending these to image on the ovelaped pixels\nThose image are ovelapping by 20-30%\n[[Pasted image 20230228153010.png]] source Seamless Mosaic (l3harrisgeospatial.com)\n[[Pasted image 20230228153057.png]] source gdal - How to create a mosaic in QGIS with cutline and feathering for Landsat-8 imagery - Geographic Information Systems Stack Exchange\nHowever the challenge is the image we get might coming from different day and lighting condition or even different satellite. It can cause different band value thus those image have to undergo standarisation and normalisation\n\nStandarisation by dividing the SR value by a maximum value per band\nnormalisation by divide the standarised value by the sum of values across all bands ## Image Enhancement\n\nit doesn’t alter the value of the data, merely changing how it explains and visual appearance ### Contrast Enhancement\n[[Pasted image 20230228153934.png]]\ndone by:\n\nstretching min max value\npercentage Linear and Standar Deviation\nPiecewise Linear Contrast Stretch\n\nit doesn’t alter the value of the data, merely changing how it explains and visual appearance\n\n\n4.2.1 Ratio enhancement\n\nband ratioing means dividing bands by each other\neg: Normalize Burn Ratio\n\n[[Pasted image 20230228154641.png]]\nsource: Landsat Normalized Burn Ratio | U.S. Geological Survey (usgs.gov)\n\n\n\n\n4.2.2 Filtering\n\nmeans taking an image an having a moving window to see aggregation of the image. Calculate surround pixel and put the average value as the middle pixel’ value\nLow Pass or low frequency is average of the surrounding pixels\nHigh pass or high frequency is enhance local vatiations\n[[Pasted image 20230228154958.png]] ### Principal Component Analysis\nUsing PCA we can make our data smaller and maximise variation between our data\nPCA will transforming multi-spectral data into uncorrelated and smaller data set\nReduce dimensionality\nExample:\n\nmulti-temporal PCA bands from both time points are combined into one image, then PCA\n[[Pasted image 20230228191626.png]]\nso initially there are 2 or 3 images from different time stamps. It was stacked together and did PCA. From the PCA,they classify land use changes. Then maximise the variation.\n\n\n\n\n4.2.3 Texture Enhancement\n\nTextture is spatial variation of gray value\nusually used for medical detection\nTexture analysis looks at the tonal feature of the image by looking at the surrounding values. So there’s a moving window with 3x3 grid and it will calculate the value of a pixel with variance and probability of surrounding (within the window) value\nTexture variance result\n\n[[Pasted image 20230228193044.png]]\nsisi yang terang adalah pinggir2 gedung karena tepi texture has high variance value\n\nTexture is beneficial to give additional information to our model as it oppose to just relying on spectral reflectance. Thus we can improve our classification model.\n\n\n\n4.2.4 Data Fusion enhancement\n\nstack of multiband data fused with PCA or texture or other enhancement.\nImage fusion can alse be from different sensor. eg. Sentinel fused with Landsat\nusually it take the median value of each pixel of each band."
  },
  {
    "objectID": "week04.html#summary-question-01",
    "href": "week04.html#summary-question-01",
    "title": "4  week04",
    "section": "4.1 Summary (Question 01)",
    "text": "4.1 Summary (Question 01)\n\n4.1.1 Annual Natural Disaster in Semarang\nSemarang is the largest and capital city of Central Java. Due to its location next to Java sea, it becomes main port of Central Java and hold crucial role for food supply. In some area of Semarang, especially coastal area, drinking water is not accessible. Limited water resources forces residents to extract groundwater which cause land subsidence approximatelly 10cm per year. Thus make them suffer from tidal flooding every year. On the other hand, Semarang has fault geological structure that makes it prone to landslide (Faizana, Nugraha and Yuwono, 2015). Local government should anticipate the risk of natural disaster to ensures the continuity of the availability resources for local community (“City Resilience Index,” no date).\n\n\n\nFlood in Semarang\n\n\nImage source: Tiga Orang Tewas Tersetrum Saat Banjir Semarang (cnnindonesia.com)\n\n\n\nSemarang Map\n\n\nImage source: Preliminary Identification of Urban Park Infrastructure Resilience in Semarang Central Java\n\n\n4.1.2 ARUP City Resilience Index\nArup has released City Resilience Index to help city government measure and monitor multiple significant factor to make a city resilience. The index are structured into 4 dimension which each of it are broken down into 3 goals with total 52 indicators.\n\n\n\nArup City Resilience Framework - Dimension, Goals, and Indicators\n\n\nUnder Infrastructure and Ecosystem dimension there are 3 goal which are reduced exposure and fragility (goals 7), effective provision of critical service (goal 8), and reliable mobility and communications (goals 9). There are some indicator that can be achieved by Semarang goverment using remote sensing data\n\n\n\nInfrastructure & Ecosystem Dimension\n\n\n\n\n\nGoals\n\n\nHere are further explanation of each indicators that can be tackled using remote sensing data\n7. REDUCED EXPOSURE & FRAGILITY\n7.1 Comprehensive hazard and exposure mapping\n\nRobust systems in place to map the city’s exposure and vulnerability to hazards based on current data.\n\n7.3 Effectively managed protective ecosystems\n\nWell-developed understanding and acknowledgement of the role of ecosystems in providing physical protection to the city.\n\n7.4 Robust protective infrastructure\n\nIntegrated, forward-looking and robust network of protective infrastructure that reduces vulnerability and exposure of citizens and critical assets.\n\n8. EFFECTIVE PROVISION OF CRITICAL SERVICES\n8.1 Effective stewardship of ecosystems\n\nRobust mechanisms are in place to maintain and enhance the ecosystem services that benefi t city residents.\n\n9. RELIABLE MOBILITY & COMMUNICATIONS\n9.2 Diverse and affordable transport networks\n\nDiverse and integrated transport networks, providing flexible and affordable travel around the city for all"
  },
  {
    "objectID": "week04.html#application-question-02",
    "href": "week04.html#application-question-02",
    "title": "4  week04",
    "section": "4.2 Application (Question 02)",
    "text": "4.2 Application (Question 02)\nTo be resilience in natural disaster, a city needs comprehensive understanding of which are vulnerable to hazard (indicator 7.1). It can be done by mapping potential natural disaster based on previous flooding and clustering map. Making flooding map can be gained by regressing SAR flooding area data, rainfall data, sea level data (from satellite data), and topography data from Shuttle Radar Mission DEM (Lin et al., 2016). To make Landslide map we can comparing historic topography data from Shuttle Radar Mission DEM which movement of geological structure (Dewan et al., 2007). Combining these 2 maps, Semarang government will can have natural disaster risk map that can be guideline for city planning to focus on areas with high risk.\nNatural disaster can cause some residential areas be isolated. Thus, emergency access are required for massive evacuation and supply daily needs which also the indicator in goals number 9 in City Resilience Index. City government need to map crucial acces and alternative acces on disaster. In order to do that, mapping or clustered residential area are required which can be obtain from classifying landsat imagery data using NDBI. By assessing residential map dan natural disaster risk map, government can make crucial emergency transportation plan. It also can be enhance with agent-based modelling to simulate evacuation and basic needs supply. Hence, government can focusing their policy and city planning in building or improving emergency transportation access.\n\n\n\nApplication Workflow Ideas"
  },
  {
    "objectID": "week04.html#reflection-question-03",
    "href": "week04.html#reflection-question-03",
    "title": "4  week04",
    "section": "4.3 Reflection (question 03)",
    "text": "4.3 Reflection (question 03)\nInternational policy help local government, that sometimes does not have a robust policy, to build and develop a resilience city. In order to achieve International policy’s target, earth observation data can be a helpful resources for decision making and city planning. Satellite imagery data allows government to have macro and micro scale observation of their historic or existing city condition. It can be beneficial to detect existing urban fabrics and potential urban network. Imagery data are also beneficial to record pre-during-post condition of an event or disaster. Thus in the future local government can prevent or protect their citizen better especially for city with periodic disaster like Semarang or Greater Dhaka (Dewan et al., 2007). Remote Sensing data allows Semarang government to achieve not only one but five indicators of ARUP City Resilience Index.\n\n\n\n\n“City Resilience Index” (no date). ARUP.\n\n\nDewan, A. M. et al. (2007) “Evaluating Flood Hazard for Land-Use Planning in Greater Dhaka of Bangladesh Using Remote Sensing and GIS Techniques,” Water Resources Management, 21(9), pp. 1601–1612. doi: 10.1007/s11269-006-9116-1.\n\n\nFaizana, F., Nugraha, A. L. and Yuwono, B. D. (2015) “Jurnal Geodesi Undip,” 4.\n\n\nLin, L. et al. (2016) “A review of remote sensing in flood assessment,” in 2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics), pp. 1–4. doi: 10.1109/Agro-Geoinformatics.2016.7577655."
  }
]